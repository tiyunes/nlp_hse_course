{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Description of the notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the current notebook I've conducted experiments of applying **Transformer** model trained from scratch for classification task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing steps:\n",
        "\n",
        "* tokenization using TweetTokenizer\n",
        "\n",
        "* lemmatization using WordNetLemmatizer\n",
        "\n",
        "* filtering out punctuation symbols and stopwords\n",
        "\n",
        "Exactly that scheme demonstrated the best results on the 1st HW task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters of the Transformer model\n",
        "\n",
        "* embedding dimension: grid = [64, 128, 256] (determines the size of token representations and influences model capacity and efficiency)\n",
        "\n",
        "* number of attention head: grid = [2, 4, 8] (controls parallel attention streams; more heads can capture diverse relationships)\n",
        "\n",
        "* number of layers: grid = [1, 2, 4] (defines model depth; deeper models may capture more complex patterns)\n",
        "\n",
        "* dropout rate = 0.3 (applies regularization to prevent overfitting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quality of the classification for different sets of hyperparameters:\n",
        "\n",
        "| embedding dim | nhead | num_layers | val Loss | val f1 score  |\n",
        "|---------------|-------|------------|----------|---------|\n",
        "| 64            | 2     | 1          | 0.6272   | 0.6094  |\n",
        "| 64            | 2     | 2          | 0.5820   | 0.6708  |\n",
        "| 64            | 2     | 4          | 0.6005   | 0.6604  |\n",
        "| 64            | 4     | 1          | 0.6220   | 0.5687  |\n",
        "| 64            | 4     | 2          | 0.6277   | 0.6347  |\n",
        "| 64            | 4     | 4          | 0.5548   | 0.6962  |\n",
        "| 64            | 8     | 1          | 0.6093   | 0.5952  |\n",
        "| 64            | 8     | 2          | 0.6033   | 0.6517  |\n",
        "| 64            | 8     | 4          | 0.5934   | 0.6581  |\n",
        "| 128           | 2     | 1          | 0.6138   | 0.6548  |\n",
        "| 128           | 2     | 2          | 0.5837   | 0.6844  |\n",
        "| 128           | 2     | 4          | 0.5337   | **0.7128**  |\n",
        "| 128           | 4     | 1          | 0.6052   | 0.6543  |\n",
        "| 128           | 4     | 2          | 0.5621   | 0.7052  |\n",
        "| 128           | 4     | 4          | 0.5753   | 0.6698  |\n",
        "| 128           | 8     | 1          | 0.5843   | 0.6786  |\n",
        "| 128           | 8     | 2          | 0.5470   | 0.6992  |\n",
        "| 128           | 8     | 4          | 0.5902   | 0.6322  |\n",
        "| 256           | 2     | 1          | 0.5930   | 0.6793  |\n",
        "| 256           | 2     | 2          | 0.6341   | 0.6352  |\n",
        "| 256           | 2     | 4          | 0.5908   | 0.6918  |\n",
        "| 256           | 4     | 1          | 0.6231   | 0.6137  |\n",
        "| 256           | 4     | 2          | 0.5596   | 0.7089  |\n",
        "| 256           | 4     | 4          | 0.5546   | 0.6908  |\n",
        "| 256           | 8     | 1          | 0.5980   | 0.6579  |\n",
        "| 256           | 8     | 2          | 0.5768   | 0.6977  |\n",
        "| 256           | 8     | 4          | 0.5566   | 0.6232  |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7X6wUbIQVlz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kjk4_z_rQVl0"
      },
      "outputs": [],
      "source": [
        "data_full = pd.read_csv('train_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "htFvY2oQQVl0",
        "outputId": "944664b0-7c0a-4a41-d257-e827649cef81"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"data_full\",\n  \"rows\": 7613,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3137,\n        \"min\": 1,\n        \"max\": 10873,\n        \"num_unique_values\": 7613,\n        \"samples\": [\n          3796,\n          3185,\n          7769\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyword\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 221,\n        \"samples\": [\n          \"injury\",\n          \"nuclear%20reactor\",\n          \"engulfed\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"location\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3341,\n        \"samples\": [\n          \"Oklahoma\",\n          \"Starling City\",\n          \"Trinidad and Tobago\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7503,\n        \"samples\": [\n          \"Three Homes Demolished in Unrecognized Arab Village - International Middle East Media Center http://t.co/ik8m4Yi9T4\",\n          \"Reid Lake fire prompts campground evacuation order http://t.co/jBODKM6rBU\",\n          \"FAAN orders evacuation of abandoned aircraft at MMA http://t.co/dEvYbnVXGQ via @todayng\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "data_full"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-8d957597-6f09-4f4b-af7e-9a8e83dd3450\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>There's an emergency evacuation happening now ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>15</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d957597-6f09-4f4b-af7e-9a8e83dd3450')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8d957597-6f09-4f4b-af7e-9a8e83dd3450 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8d957597-6f09-4f4b-af7e-9a8e83dd3450');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-851ef4fe-abac-4f06-9b71-079eecabfe88\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-851ef4fe-abac-4f06-9b71-079eecabfe88')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-851ef4fe-abac-4f06-9b71-079eecabfe88 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
              "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
              "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
              "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
              "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  \n",
              "5       1  \n",
              "6       1  \n",
              "7       1  \n",
              "8       1  \n",
              "9       1  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_full.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSrQ5xGdQVl1"
      },
      "outputs": [],
      "source": [
        "data = data_full[['text', 'target']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGcTOuDUQVl1"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TweetTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUANG7fhQVl1"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qURpXbvZQq6i",
        "outputId": "909af691-0c45-4ad8-baf8-ca5bdb2fde72"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtkeA2igQVl2"
      },
      "outputs": [],
      "source": [
        "tknzr = TweetTokenizer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1q7bfQRQVl2"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_lemmatize(text):\n",
        "    tokens = tknzr.tokenize(text)\n",
        "    return list(map(lemmatizer.lemmatize, tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJDmGyr0QVl2",
        "outputId": "0f1f0d51-1fe5-4099-dbe5-34a5348ba8ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-670c2ebe6ef7>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['tokenized_text'] = data['text'].apply(\n"
          ]
        }
      ],
      "source": [
        "data['tokenized_text'] = data['text'].apply(\n",
        "    lambda sent: tokenize_and_lemmatize(sent)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6qy_ORkQVl2",
        "outputId": "32871f94-6887-4851-a5b2-011a308a3d1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords', quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sJbDlyYQVl3"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYtPrJV8QVl3"
      },
      "outputs": [],
      "source": [
        "stopwords_set = set(stopwords.words(\"english\"))\n",
        "punctuation_set = set(punctuation)\n",
        "noise = stopwords_set.union(punctuation_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZDuIcw0QVl3",
        "outputId": "c9388491-46bc-4d93-97e8-775cd9eebc5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-2e1a004b0f04>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['filtered_text'] = data['tokenized_text'].apply(\n"
          ]
        }
      ],
      "source": [
        "data['filtered_text'] = data['tokenized_text'].apply(\n",
        "    lambda tokens: [token.lower() for token in tokens if token.lower() not in noise]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krj07mQOQVl3"
      },
      "outputs": [],
      "source": [
        "data['filtered_text_joined'] = data['filtered_text'].apply(lambda tokens: ' '.join(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "5T17gwCDQVl4",
        "outputId": "d4cedc43-f930-456c-f773-2df2e25bdea0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 7613,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7503,\n        \"samples\": [\n          \"Three Homes Demolished in Unrecognized Arab Village - International Middle East Media Center http://t.co/ik8m4Yi9T4\",\n          \"Reid Lake fire prompts campground evacuation order http://t.co/jBODKM6rBU\",\n          \"FAAN orders evacuation of abandoned aircraft at MMA http://t.co/dEvYbnVXGQ via @todayng\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized_text\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"filtered_text\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"filtered_text_joined\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7488,\n        \"samples\": [\n          \"tips finding customer ego drought dqsvyusy\",\n          \"day drown tear let get\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-2295dd66-fb25-4e48-96e6-d1e006c65bbc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>tokenized_text</th>\n",
              "      <th>filtered_text</th>\n",
              "      <th>filtered_text_joined</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>[Our, Deeds, are, the, Reason, of, this, #eart...</td>\n",
              "      <td>[deeds, reason, #earthquake, may, allah, forgi...</td>\n",
              "      <td>deeds reason #earthquake may allah forgive u</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>[Forest, fire, near, La, Ronge, Sask, ., Canada]</td>\n",
              "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
              "      <td>forest fire near la ronge sask canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[All, resident, asked, to, ', shelter, in, pla...</td>\n",
              "      <td>[resident, asked, shelter, place, notified, of...</td>\n",
              "      <td>resident asked shelter place notified officer ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>[13,000, people, receive, #wildfires, evacuati...</td>\n",
              "      <td>[13,000, people, receive, #wildfires, evacuati...</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[Just, got, sent, this, photo, from, Ruby, #Al...</td>\n",
              "      <td>[got, sent, photo, ruby, #alaska, smoke, #wild...</td>\n",
              "      <td>got sent photo ruby #alaska smoke #wildfires p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7608</th>\n",
              "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
              "      <td>1</td>\n",
              "      <td>[Two, giant, crane, holding, a, bridge, collap...</td>\n",
              "      <td>[two, giant, crane, holding, bridge, collapse,...</td>\n",
              "      <td>two giant crane holding bridge collapse nearby...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7609</th>\n",
              "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
              "      <td>1</td>\n",
              "      <td>[@aria_ahrary, @TheTawniest, The, out, of, con...</td>\n",
              "      <td>[@aria_ahrary, @thetawniest, control, wild, fi...</td>\n",
              "      <td>@aria_ahrary @thetawniest control wild fire ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7610</th>\n",
              "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
              "      <td>1</td>\n",
              "      <td>[M1, ., 94, [, 01:04, UTC, ], ?, 5km, S, of, V...</td>\n",
              "      <td>[m1, 94, 01:04, utc, 5km, volcano, hawaii, htt...</td>\n",
              "      <td>m1 94 01:04 utc 5km volcano hawaii http://t.co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7611</th>\n",
              "      <td>Police investigating after an e-bike collided ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[Police, investigating, after, an, e-bike, col...</td>\n",
              "      <td>[police, investigating, e-bike, collided, car,...</td>\n",
              "      <td>police investigating e-bike collided car littl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7612</th>\n",
              "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
              "      <td>1</td>\n",
              "      <td>[The, Latest, :, More, Homes, Razed, by, North...</td>\n",
              "      <td>[latest, homes, razed, northern, california, w...</td>\n",
              "      <td>latest homes razed northern california wildfir...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7613 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2295dd66-fb25-4e48-96e6-d1e006c65bbc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2295dd66-fb25-4e48-96e6-d1e006c65bbc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2295dd66-fb25-4e48-96e6-d1e006c65bbc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b8c41a82-7858-42d9-9cab-90be064b1ef0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b8c41a82-7858-42d9-9cab-90be064b1ef0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b8c41a82-7858-42d9-9cab-90be064b1ef0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                   text  target  \\\n",
              "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
              "1                Forest fire near La Ronge Sask. Canada       1   \n",
              "2     All residents asked to 'shelter in place' are ...       1   \n",
              "3     13,000 people receive #wildfires evacuation or...       1   \n",
              "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
              "...                                                 ...     ...   \n",
              "7608  Two giant cranes holding a bridge collapse int...       1   \n",
              "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
              "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
              "7611  Police investigating after an e-bike collided ...       1   \n",
              "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
              "\n",
              "                                         tokenized_text  \\\n",
              "0     [Our, Deeds, are, the, Reason, of, this, #eart...   \n",
              "1      [Forest, fire, near, La, Ronge, Sask, ., Canada]   \n",
              "2     [All, resident, asked, to, ', shelter, in, pla...   \n",
              "3     [13,000, people, receive, #wildfires, evacuati...   \n",
              "4     [Just, got, sent, this, photo, from, Ruby, #Al...   \n",
              "...                                                 ...   \n",
              "7608  [Two, giant, crane, holding, a, bridge, collap...   \n",
              "7609  [@aria_ahrary, @TheTawniest, The, out, of, con...   \n",
              "7610  [M1, ., 94, [, 01:04, UTC, ], ?, 5km, S, of, V...   \n",
              "7611  [Police, investigating, after, an, e-bike, col...   \n",
              "7612  [The, Latest, :, More, Homes, Razed, by, North...   \n",
              "\n",
              "                                          filtered_text  \\\n",
              "0     [deeds, reason, #earthquake, may, allah, forgi...   \n",
              "1         [forest, fire, near, la, ronge, sask, canada]   \n",
              "2     [resident, asked, shelter, place, notified, of...   \n",
              "3     [13,000, people, receive, #wildfires, evacuati...   \n",
              "4     [got, sent, photo, ruby, #alaska, smoke, #wild...   \n",
              "...                                                 ...   \n",
              "7608  [two, giant, crane, holding, bridge, collapse,...   \n",
              "7609  [@aria_ahrary, @thetawniest, control, wild, fi...   \n",
              "7610  [m1, 94, 01:04, utc, 5km, volcano, hawaii, htt...   \n",
              "7611  [police, investigating, e-bike, collided, car,...   \n",
              "7612  [latest, homes, razed, northern, california, w...   \n",
              "\n",
              "                                   filtered_text_joined  \n",
              "0          deeds reason #earthquake may allah forgive u  \n",
              "1                 forest fire near la ronge sask canada  \n",
              "2     resident asked shelter place notified officer ...  \n",
              "3     13,000 people receive #wildfires evacuation or...  \n",
              "4     got sent photo ruby #alaska smoke #wildfires p...  \n",
              "...                                                 ...  \n",
              "7608  two giant crane holding bridge collapse nearby...  \n",
              "7609  @aria_ahrary @thetawniest control wild fire ca...  \n",
              "7610  m1 94 01:04 utc 5km volcano hawaii http://t.co...  \n",
              "7611  police investigating e-bike collided car littl...  \n",
              "7612  latest homes razed northern california wildfir...  \n",
              "\n",
              "[7613 rows x 5 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urUbCByKQVl4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bLPE5OOQVl4"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(data['filtered_text_joined'], data['target'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7W2WJ4MNQVl4"
      },
      "outputs": [],
      "source": [
        "def build_vocab(texts, max_words=20000):\n",
        "    token_count_dict = {}\n",
        "    for text in texts:\n",
        "        for token in text.split():\n",
        "            if token not in token_count_dict:\n",
        "                token_count_dict[token] = 1\n",
        "            else:\n",
        "                token_count_dict[token] = token_count_dict[token] + 1\n",
        "\n",
        "    tokens_freq_list = list(token_count_dict.items())\n",
        "    tokens_freq_list.sort(key=lambda x: x[1], reverse=True)\n",
        "    sorted_tokens = tokens_freq_list[:max_words - 2]\n",
        "\n",
        "    vocabulary = {\n",
        "        \"<pad>\": 0,\n",
        "        \"<oov>\": 1,\n",
        "    }\n",
        "\n",
        "    for i, (token, count) in enumerate(sorted_tokens):\n",
        "        vocabulary[token] = i\n",
        "\n",
        "    return vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4wXrENEQVl4"
      },
      "outputs": [],
      "source": [
        "vocab = build_vocab(data['filtered_text_joined'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dx1rnshQVl5",
        "outputId": "e6a03a72-b7c8-49d8-99e5-3e46edf31fa8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'<pad>': 0,\n",
              " '<oov>': 1,\n",
              " '...': 0,\n",
              " '\\x89': 1,\n",
              " 'wa': 2,\n",
              " 'like': 3,\n",
              " 'û_': 4,\n",
              " 'fire': 5,\n",
              " 'get': 6,\n",
              " 'ha': 7,\n",
              " 'new': 8,\n",
              " 'via': 9,\n",
              " 'one': 10,\n",
              " 'u': 11,\n",
              " 'people': 12,\n",
              " '2': 13,\n",
              " 'video': 14,\n",
              " 'emergency': 15,\n",
              " 'disaster': 16,\n",
              " 'time': 17,\n",
              " 'body': 18,\n",
              " 'police': 19,\n",
              " 'day': 20,\n",
              " 'year': 21,\n",
              " 'would': 22,\n",
              " 'still': 23,\n",
              " 'building': 24,\n",
              " 'say': 25,\n",
              " 'go': 26,\n",
              " 'news': 27,\n",
              " 'home': 28,\n",
              " 'crash': 29,\n",
              " 'storm': 30,\n",
              " 'back': 31,\n",
              " '..': 32,\n",
              " 'burning': 33,\n",
              " 'know': 34,\n",
              " 'suicide': 35,\n",
              " '3': 36,\n",
              " 'got': 37,\n",
              " 'california': 38,\n",
              " 'see': 39,\n",
              " 'man': 40,\n",
              " 'car': 41,\n",
              " 'look': 42,\n",
              " 'first': 43,\n",
              " 'attack': 44,\n",
              " 'life': 45,\n",
              " 'death': 46,\n",
              " 'bomb': 47,\n",
              " 'train': 48,\n",
              " 'going': 49,\n",
              " 'make': 50,\n",
              " 'love': 51,\n",
              " 'family': 52,\n",
              " 'rt': 53,\n",
              " 'two': 54,\n",
              " 'killed': 55,\n",
              " 'world': 56,\n",
              " 'dead': 57,\n",
              " 'flood': 58,\n",
              " 'û': 59,\n",
              " 'accident': 60,\n",
              " 'nuclear': 61,\n",
              " 'today': 62,\n",
              " 'full': 63,\n",
              " 'want': 64,\n",
              " 'war': 65,\n",
              " 'need': 66,\n",
              " 'good': 67,\n",
              " 'think': 68,\n",
              " 'may': 69,\n",
              " \"can't\": 70,\n",
              " 'way': 71,\n",
              " 'pm': 72,\n",
              " 'watch': 73,\n",
              " 'ûªs': 74,\n",
              " 'many': 75,\n",
              " 'last': 76,\n",
              " '@youtube': 77,\n",
              " 'injury': 78,\n",
              " 'take': 79,\n",
              " 'could': 80,\n",
              " 'collapse': 81,\n",
              " 'w': 82,\n",
              " 'let': 83,\n",
              " 'work': 84,\n",
              " '#news': 85,\n",
              " 'help': 86,\n",
              " '5': 87,\n",
              " '4': 88,\n",
              " 'even': 89,\n",
              " 'weapon': 90,\n",
              " 'mass': 91,\n",
              " 'please': 92,\n",
              " 'another': 93,\n",
              " 'right': 94,\n",
              " 'really': 95,\n",
              " '2015': 96,\n",
              " 'army': 97,\n",
              " '1': 98,\n",
              " 'come': 99,\n",
              " 'lol': 100,\n",
              " 'bombing': 101,\n",
              " 'hiroshima': 102,\n",
              " 'black': 103,\n",
              " 'bag': 104,\n",
              " 'school': 105,\n",
              " 'wildfire': 106,\n",
              " 'city': 107,\n",
              " 'fatality': 108,\n",
              " 'thing': 109,\n",
              " 'read': 110,\n",
              " 'fatal': 111,\n",
              " 'mh370': 112,\n",
              " 'forest': 113,\n",
              " 'much': 114,\n",
              " 'northern': 115,\n",
              " 'water': 116,\n",
              " 'plan': 117,\n",
              " 'great': 118,\n",
              " 'feel': 119,\n",
              " 'bomber': 120,\n",
              " 'never': 121,\n",
              " 'obama': 122,\n",
              " 'fear': 123,\n",
              " 'hostage': 124,\n",
              " 'legionnaires': 125,\n",
              " 'damage': 126,\n",
              " 'wreck': 127,\n",
              " 'latest': 128,\n",
              " 'live': 129,\n",
              " 'every': 130,\n",
              " 'kill': 131,\n",
              " 'hit': 132,\n",
              " 'woman': 133,\n",
              " 'old': 134,\n",
              " 'cause': 135,\n",
              " 'stop': 136,\n",
              " 'atomic': 137,\n",
              " 'top': 138,\n",
              " 'wave': 139,\n",
              " 'said': 140,\n",
              " 'getting': 141,\n",
              " 'shit': 142,\n",
              " 'us': 143,\n",
              " 'house': 144,\n",
              " 'service': 145,\n",
              " 'report': 146,\n",
              " 'injured': 147,\n",
              " 'im': 148,\n",
              " 'near': 149,\n",
              " 'ever': 150,\n",
              " 'since': 151,\n",
              " 'hope': 152,\n",
              " 'wind': 153,\n",
              " 'everyone': 154,\n",
              " \"that's\": 155,\n",
              " 'content': 156,\n",
              " 'coming': 157,\n",
              " 'night': 158,\n",
              " 'truck': 159,\n",
              " 'found': 160,\n",
              " 'state': 161,\n",
              " 'casualty': 162,\n",
              " 'thunderstorm': 163,\n",
              " 'evacuation': 164,\n",
              " 'rain': 165,\n",
              " 'area': 166,\n",
              " 'next': 167,\n",
              " 'without': 168,\n",
              " 'movie': 169,\n",
              " 'fall': 170,\n",
              " 'oil': 171,\n",
              " 'end': 172,\n",
              " 'debris': 173,\n",
              " 'girl': 174,\n",
              " 'warning': 175,\n",
              " 'show': 176,\n",
              " 'cross': 177,\n",
              " 'smoke': 178,\n",
              " 'set': 179,\n",
              " 'god': 180,\n",
              " 'game': 181,\n",
              " '÷': 182,\n",
              " 'update': 183,\n",
              " 'face': 184,\n",
              " 'little': 185,\n",
              " 'run': 186,\n",
              " 'ûª': 187,\n",
              " 'weather': 188,\n",
              " 'wounded': 189,\n",
              " 'malaysia': 190,\n",
              " 'head': 191,\n",
              " 'call': 192,\n",
              " 'food': 193,\n",
              " 'wild': 194,\n",
              " 'boy': 195,\n",
              " 'confirmed': 196,\n",
              " 'severe': 197,\n",
              " 'flooding': 198,\n",
              " 'heat': 199,\n",
              " 'always': 200,\n",
              " 'check': 201,\n",
              " 'fucking': 202,\n",
              " 'well': 203,\n",
              " 'military': 204,\n",
              " 'story': 205,\n",
              " 'keep': 206,\n",
              " 'flame': 207,\n",
              " '70': 208,\n",
              " 'bad': 209,\n",
              " 'sound': 210,\n",
              " 'migrant': 211,\n",
              " 'road': 212,\n",
              " 'also': 213,\n",
              " 'bloody': 214,\n",
              " 'murder': 215,\n",
              " 'blood': 216,\n",
              " 'high': 217,\n",
              " 'liked': 218,\n",
              " 'natural': 219,\n",
              " 'sinking': 220,\n",
              " 'spill': 221,\n",
              " 'loud': 222,\n",
              " 'photo': 223,\n",
              " 'gonna': 224,\n",
              " 'bus': 225,\n",
              " 'best': 226,\n",
              " 'screaming': 227,\n",
              " '11': 228,\n",
              " 'fan': 229,\n",
              " 'japan': 230,\n",
              " 'missing': 231,\n",
              " 'post': 232,\n",
              " 'made': 233,\n",
              " 'week': 234,\n",
              " 'ûò': 235,\n",
              " 'explosion': 236,\n",
              " 'air': 237,\n",
              " 'save': 238,\n",
              " 'child': 239,\n",
              " 'outbreak': 240,\n",
              " 'lightning': 241,\n",
              " 'bridge': 242,\n",
              " 'evacuate': 243,\n",
              " 'failure': 244,\n",
              " 'trapped': 245,\n",
              " 'collided': 246,\n",
              " 'summer': 247,\n",
              " 'someone': 248,\n",
              " 'minute': 249,\n",
              " 'guy': 250,\n",
              " 'lot': 251,\n",
              " 'whole': 252,\n",
              " 'survive': 253,\n",
              " 'hail': 254,\n",
              " 'released': 255,\n",
              " 'attacked': 256,\n",
              " 'panic': 257,\n",
              " 'explode': 258,\n",
              " 'derailment': 259,\n",
              " 'harm': 260,\n",
              " 'rescue': 261,\n",
              " 'thunder': 262,\n",
              " 'reddit': 263,\n",
              " 'wreckage': 264,\n",
              " 'around': 265,\n",
              " 'tonight': 266,\n",
              " '6': 267,\n",
              " 'ambulance': 268,\n",
              " 'sign': 269,\n",
              " 'part': 270,\n",
              " 'destroyed': 271,\n",
              " 'fuck': 272,\n",
              " 'big': 273,\n",
              " 'terrorist': 274,\n",
              " 'hurricane': 275,\n",
              " 'siren': 276,\n",
              " 'hazard': 277,\n",
              " 'burned': 278,\n",
              " 'free': 279,\n",
              " 'island': 280,\n",
              " 'destroy': 281,\n",
              " 'hour': 282,\n",
              " 'friend': 283,\n",
              " 'charged': 284,\n",
              " 'ruin': 285,\n",
              " 'rescued': 286,\n",
              " 'sinkhole': 287,\n",
              " 'county': 288,\n",
              " \"there's\": 289,\n",
              " 'tornado': 290,\n",
              " '15': 291,\n",
              " '\\x9d': 292,\n",
              " 'trauma': 293,\n",
              " 'destruction': 294,\n",
              " 'away': 295,\n",
              " 'white': 296,\n",
              " 'battle': 297,\n",
              " 'put': 298,\n",
              " 'baby': 299,\n",
              " 'boat': 300,\n",
              " 'crush': 301,\n",
              " 'dust': 302,\n",
              " 'survived': 303,\n",
              " 'twister': 304,\n",
              " 'wrecked': 305,\n",
              " 'officer': 306,\n",
              " 'order': 307,\n",
              " 'heart': 308,\n",
              " 'issue': 309,\n",
              " 'p': 310,\n",
              " 'suspect': 311,\n",
              " 'crashed': 312,\n",
              " 'phone': 313,\n",
              " '05': 314,\n",
              " 'light': 315,\n",
              " 'violent': 316,\n",
              " 'real': 317,\n",
              " 'saudi': 318,\n",
              " 'group': 319,\n",
              " 'eye': 320,\n",
              " 'word': 321,\n",
              " 'riot': 322,\n",
              " 'cliff': 323,\n",
              " 'curfew': 324,\n",
              " 'investigators': 325,\n",
              " 'massacre': 326,\n",
              " 'quarantine': 327,\n",
              " 'structural': 328,\n",
              " 'sandstorm': 329,\n",
              " 'sunk': 330,\n",
              " 'windstorm': 331,\n",
              " 'better': 332,\n",
              " 'least': 333,\n",
              " 'market': 334,\n",
              " 'kid': 335,\n",
              " '40': 336,\n",
              " 'came': 337,\n",
              " 'plane': 338,\n",
              " 'national': 339,\n",
              " 'august': 340,\n",
              " 'red': 341,\n",
              " 'hot': 342,\n",
              " 'start': 343,\n",
              " 'saw': 344,\n",
              " 'twitter': 345,\n",
              " 'power': 346,\n",
              " 'change': 347,\n",
              " 'land': 348,\n",
              " 'displaced': 349,\n",
              " 'catastrophe': 350,\n",
              " 'hazardous': 351,\n",
              " 'collapsed': 352,\n",
              " 'collision': 353,\n",
              " 'danger': 354,\n",
              " 'stock': 355,\n",
              " 'devastation': 356,\n",
              " 'drowning': 357,\n",
              " 'engulfed': 358,\n",
              " 'screamed': 359,\n",
              " 'famine': 360,\n",
              " 'bang': 361,\n",
              " 'quarantined': 362,\n",
              " 'past': 363,\n",
              " 'heard': 364,\n",
              " 'thought': 365,\n",
              " 'long': 366,\n",
              " 'mosque': 367,\n",
              " 'wanna': 368,\n",
              " 'oh': 369,\n",
              " 'bleeding': 370,\n",
              " 'tragedy': 371,\n",
              " 'anniversary': 372,\n",
              " 'bombed': 373,\n",
              " 'landslide': 374,\n",
              " 'deluge': 375,\n",
              " 'derail': 376,\n",
              " 'exploded': 377,\n",
              " 'inundated': 378,\n",
              " 'whirlwind': 379,\n",
              " 'use': 380,\n",
              " 'horrible': 381,\n",
              " '9': 382,\n",
              " 'meltdown': 383,\n",
              " 'ok': 384,\n",
              " 'went': 385,\n",
              " 'blast': 386,\n",
              " 'pic': 387,\n",
              " 'blew': 388,\n",
              " 'drown': 389,\n",
              " 'blown': 390,\n",
              " 'rioting': 391,\n",
              " 'traumatised': 392,\n",
              " 'derailed': 393,\n",
              " 'desolation': 394,\n",
              " 'trouble': 395,\n",
              " 'detonate': 396,\n",
              " 'electrocuted': 397,\n",
              " 'rescuers': 398,\n",
              " 'shot': 399,\n",
              " 'something': 400,\n",
              " 'n': 401,\n",
              " 'thank': 402,\n",
              " 'left': 403,\n",
              " 'reunion': 404,\n",
              " 'government': 405,\n",
              " 'fight': 406,\n",
              " 'must': 407,\n",
              " 'zone': 408,\n",
              " 'soon': 409,\n",
              " 'half': 410,\n",
              " 'affected': 411,\n",
              " 'collide': 412,\n",
              " 'typhoon': 413,\n",
              " 'wound': 414,\n",
              " 'earthquake': 415,\n",
              " 'evacuated': 416,\n",
              " 'flattened': 417,\n",
              " 'hijacker': 418,\n",
              " 'lava': 419,\n",
              " 'mudslide': 420,\n",
              " 'panicking': 421,\n",
              " 'street': 422,\n",
              " 'possible': 423,\n",
              " 'airplane': 424,\n",
              " 'official': 425,\n",
              " 'ship': 426,\n",
              " 'iran': 427,\n",
              " 'river': 428,\n",
              " 'apocalypse': 429,\n",
              " 'tomorrow': 430,\n",
              " 'b': 431,\n",
              " '8': 432,\n",
              " 'shoulder': 433,\n",
              " 'stay': 434,\n",
              " 'send': 435,\n",
              " 'drought': 436,\n",
              " 'catastrophic': 437,\n",
              " 'demolish': 438,\n",
              " 'hijacking': 439,\n",
              " 'razed': 440,\n",
              " 'place': 441,\n",
              " 'due': 442,\n",
              " 'second': 443,\n",
              " 'breaking': 444,\n",
              " 'annihilated': 445,\n",
              " 'case': 446,\n",
              " 'survivor': 447,\n",
              " 'arson': 448,\n",
              " 'sure': 449,\n",
              " 'fedex': 450,\n",
              " 'longer': 451,\n",
              " 'blazing': 452,\n",
              " 'song': 453,\n",
              " 'ur': 454,\n",
              " 'bagging': 455,\n",
              " 'pkk': 456,\n",
              " 'caused': 457,\n",
              " 'crushed': 458,\n",
              " '#hot': 459,\n",
              " 'detonation': 460,\n",
              " 'refugee': 461,\n",
              " '2015-08-': 462,\n",
              " 'obliterated': 463,\n",
              " 'pandemonium': 464,\n",
              " 'detonated': 465,\n",
              " 'thanks': 466,\n",
              " 'care': 467,\n",
              " 'used': 468,\n",
              " 'airport': 469,\n",
              " 'book': 470,\n",
              " 'armageddon': 471,\n",
              " 'yet': 472,\n",
              " 'person': 473,\n",
              " 'beautiful': 474,\n",
              " '7': 475,\n",
              " 'bioterror': 476,\n",
              " 'lab': 477,\n",
              " 'nothing': 478,\n",
              " 'murderer': 479,\n",
              " 'chemical': 480,\n",
              " 'drowned': 481,\n",
              " '#prebreak': 482,\n",
              " '#best': 483,\n",
              " 'obliterate': 484,\n",
              " 'three': 485,\n",
              " 'south': 486,\n",
              " 'cool': 487,\n",
              " 'st': 488,\n",
              " 'inside': 489,\n",
              " 'site': 490,\n",
              " 'r': 491,\n",
              " 'tell': 492,\n",
              " 'support': 493,\n",
              " 'shooting': 494,\n",
              " 'already': 495,\n",
              " 'making': 496,\n",
              " 'done': 497,\n",
              " 'play': 498,\n",
              " 'believe': 499,\n",
              " 'ebay': 500,\n",
              " '10': 501,\n",
              " 'remember': 502,\n",
              " 'families': 503,\n",
              " 'calgary': 504,\n",
              " 'security': 505,\n",
              " 'c': 506,\n",
              " 'volcano': 507,\n",
              " 'rise': 508,\n",
              " 'demolished': 509,\n",
              " 'electrocute': 510,\n",
              " 'eyewitness': 511,\n",
              " 'obliteration': 512,\n",
              " 'scream': 513,\n",
              " 'upheaval': 514,\n",
              " 'died': 515,\n",
              " 'far': 516,\n",
              " 'ûó': 517,\n",
              " 'leave': 518,\n",
              " 'traffic': 519,\n",
              " 'actually': 520,\n",
              " 'men': 521,\n",
              " 'ûªt': 522,\n",
              " 'wake': 523,\n",
              " 'move': 524,\n",
              " 'terrorism': 525,\n",
              " 'team': 526,\n",
              " 'policy': 527,\n",
              " 'find': 528,\n",
              " 'turkey': 529,\n",
              " 'bush': 530,\n",
              " 'cyclone': 531,\n",
              " 'demolition': 532,\n",
              " 'hundred': 533,\n",
              " 'tsunami': 534,\n",
              " 'hijack': 535,\n",
              " 'sue': 536,\n",
              " '16yr': 537,\n",
              " 'reason': 538,\n",
              " 'ablaze': 539,\n",
              " 'north': 540,\n",
              " 'mean': 541,\n",
              " '30': 542,\n",
              " 'doe': 543,\n",
              " 'die': 544,\n",
              " 'services': 545,\n",
              " 'nigga': 546,\n",
              " 'country': 547,\n",
              " 'fun': 548,\n",
              " 'bar': 549,\n",
              " 'v': 550,\n",
              " 'peace': 551,\n",
              " 'times': 552,\n",
              " 'israeli': 553,\n",
              " 'trying': 554,\n",
              " 'feeling': 555,\n",
              " 'music': 556,\n",
              " 'deal': 557,\n",
              " 'line': 558,\n",
              " 'effect': 559,\n",
              " 'nearby': 560,\n",
              " 'deluged': 561,\n",
              " 'declares': 562,\n",
              " 'year-old': 563,\n",
              " 'reactor': 564,\n",
              " 'rainstorm': 565,\n",
              " 'snowstorm': 566,\n",
              " 'wait': 567,\n",
              " 'side': 568,\n",
              " 'win': 569,\n",
              " 'brown': 570,\n",
              " 'ago': 571,\n",
              " 'helicopter': 572,\n",
              " 'dog': 573,\n",
              " 'horror': 574,\n",
              " 'anything': 575,\n",
              " 'gun': 576,\n",
              " 'yeah': 577,\n",
              " '50': 578,\n",
              " 'blight': 579,\n",
              " 'mp': 580,\n",
              " 'memory': 581,\n",
              " 'seismic': 582,\n",
              " 'offensive': 583,\n",
              " 'rubble': 584,\n",
              " 'swallowed': 585,\n",
              " 'lost': 586,\n",
              " \"what's\": 587,\n",
              " 'outside': 588,\n",
              " 'west': 589,\n",
              " 'almost': 590,\n",
              " 'hey': 591,\n",
              " 'hell': 592,\n",
              " 'maybe': 593,\n",
              " 'data': 594,\n",
              " 'american': 595,\n",
              " 'action': 596,\n",
              " 'health': 597,\n",
              " 'yes': 598,\n",
              " 'watching': 599,\n",
              " 'bigger': 600,\n",
              " '25': 601,\n",
              " 'typhoon-devastated': 602,\n",
              " 'saipan': 603,\n",
              " 'hat': 604,\n",
              " 'hellfire': 605,\n",
              " 'conclusively': 606,\n",
              " 'center': 607,\n",
              " 'america': 608,\n",
              " 'stand': 609,\n",
              " 'anyone': 610,\n",
              " 'victim': 611,\n",
              " 'bc': 612,\n",
              " 'name': 613,\n",
              " 'hand': 614,\n",
              " '):': 615,\n",
              " 'pick': 616,\n",
              " 'control': 617,\n",
              " 'literally': 618,\n",
              " 'tweet': 619,\n",
              " 'star': 620,\n",
              " 'transport': 621,\n",
              " 'bioterrorism': 622,\n",
              " 'e': 623,\n",
              " 'computer': 624,\n",
              " 'searching': 625,\n",
              " 'level': 626,\n",
              " 'low': 627,\n",
              " 'crew': 628,\n",
              " 'hear': 629,\n",
              " 'desolate': 630,\n",
              " 'utc': 631,\n",
              " 'projected': 632,\n",
              " 'month': 633,\n",
              " '#nowplaying': 634,\n",
              " 'talk': 635,\n",
              " 'vehicle': 636,\n",
              " 'rd': 637,\n",
              " 'mom': 638,\n",
              " 'finally': 639,\n",
              " 'might': 640,\n",
              " 'everything': 641,\n",
              " 'history': 642,\n",
              " 'ball': 643,\n",
              " 'point': 644,\n",
              " 'amid': 645,\n",
              " 'blaze': 646,\n",
              " 'million': 647,\n",
              " 'damn': 648,\n",
              " 'avalanche': 649,\n",
              " 'hollywood': 650,\n",
              " 'pretty': 651,\n",
              " 'online': 652,\n",
              " 'pay': 653,\n",
              " 'though': 654,\n",
              " 'link': 655,\n",
              " 'probably': 656,\n",
              " 'spot': 657,\n",
              " 'saved': 658,\n",
              " 'responder': 659,\n",
              " 'china': 660,\n",
              " 'soudelor': 661,\n",
              " 'aircraft': 662,\n",
              " '#islam': 663,\n",
              " 'manslaughter': 664,\n",
              " 'trench': 665,\n",
              " 'fast': 666,\n",
              " 'try': 667,\n",
              " 'happy': 668,\n",
              " 'feared': 669,\n",
              " 'couple': 670,\n",
              " 'seen': 671,\n",
              " 'share': 672,\n",
              " 'major': 673,\n",
              " 'class': 674,\n",
              " 'crisis': 675,\n",
              " 'tv': 676,\n",
              " 'leather': 677,\n",
              " 'caught': 678,\n",
              " 'town': 679,\n",
              " 'okay': 680,\n",
              " 'youth': 681,\n",
              " 'space': 682,\n",
              " 'problem': 683,\n",
              " 'cake': 684,\n",
              " 'rock': 685,\n",
              " 'money': 686,\n",
              " '#hiroshima': 687,\n",
              " 'tree': 688,\n",
              " 'abc': 689,\n",
              " 'refugio': 690,\n",
              " 'costlier': 691,\n",
              " 'la': 692,\n",
              " '20': 693,\n",
              " 'flash': 694,\n",
              " 'flag': 695,\n",
              " 'huge': 696,\n",
              " 'daily': 697,\n",
              " 'wrong': 698,\n",
              " 'crazy': 699,\n",
              " 'hate': 700,\n",
              " 'annihilation': 701,\n",
              " 'self': 702,\n",
              " 'called': 703,\n",
              " 'blue': 704,\n",
              " 'east': 705,\n",
              " 'business': 706,\n",
              " 'texas': 707,\n",
              " 'nearly': 708,\n",
              " 'morning': 709,\n",
              " 'giant': 710,\n",
              " 'village': 711,\n",
              " '60': 712,\n",
              " 'alarm': 713,\n",
              " 'chance': 714,\n",
              " 'banned': 715,\n",
              " 'knock': 716,\n",
              " 'picking': 717,\n",
              " 'closed': 718,\n",
              " 'across': 719,\n",
              " 'haha': 720,\n",
              " 'lord': 721,\n",
              " 'hard': 722,\n",
              " 'others': 723,\n",
              " 'property': 724,\n",
              " 'drive': 725,\n",
              " 'omg': 726,\n",
              " 'sorry': 727,\n",
              " 'poor': 728,\n",
              " 'toddler': 729,\n",
              " 'united': 730,\n",
              " 'india': 731,\n",
              " 'worst': 732,\n",
              " 'view': 733,\n",
              " 'mishap': 734,\n",
              " 'dont': 735,\n",
              " 'turn': 736,\n",
              " \"let's\": 737,\n",
              " 'entire': 738,\n",
              " 'wow': 739,\n",
              " 'meek': 740,\n",
              " 'aug': 741,\n",
              " '06': 742,\n",
              " 'photos': 743,\n",
              " 'camp': 744,\n",
              " 'become': 745,\n",
              " 'company': 746,\n",
              " 'homes': 747,\n",
              " 'angry': 748,\n",
              " 'ignition': 749,\n",
              " 'devastated': 750,\n",
              " '#earthquake': 751,\n",
              " 'heavy': 752,\n",
              " ':)': 753,\n",
              " 'wanted': 754,\n",
              " 'alone': 755,\n",
              " '16': 756,\n",
              " 'happened': 757,\n",
              " 'aftershock': 758,\n",
              " 'member': 759,\n",
              " '12': 760,\n",
              " 'driver': 761,\n",
              " 'soul': 762,\n",
              " 'film': 763,\n",
              " 'totally': 764,\n",
              " 'give': 765,\n",
              " 'lead': 766,\n",
              " 'learn': 767,\n",
              " 'beach': 768,\n",
              " 'officials': 769,\n",
              " 'force': 770,\n",
              " 'christian': 771,\n",
              " 'temple': 772,\n",
              " 'favorite': 773,\n",
              " 'taken': 774,\n",
              " 'playing': 775,\n",
              " 'anthrax': 776,\n",
              " 'public': 777,\n",
              " 'running': 778,\n",
              " 'looking': 779,\n",
              " 'mad': 780,\n",
              " 'pain': 781,\n",
              " 'open': 782,\n",
              " 'blizzard': 783,\n",
              " 'idea': 784,\n",
              " 'large': 785,\n",
              " 'appears': 786,\n",
              " 'listen': 787,\n",
              " 'centre': 788,\n",
              " 'human': 789,\n",
              " 'issued': 790,\n",
              " 'flight': 791,\n",
              " 'declaration': 792,\n",
              " 'disea': 793,\n",
              " 'front': 794,\n",
              " 'else': 795,\n",
              " 'risk': 796,\n",
              " 'comment': 797,\n",
              " 'moment': 798,\n",
              " 'cop': 799,\n",
              " 'pakistan': 800,\n",
              " 'ûï': 801,\n",
              " 'number': 802,\n",
              " 'trust': 803,\n",
              " 'ready': 804,\n",
              " 'park': 805,\n",
              " 'disease': 806,\n",
              " 'mountain': 807,\n",
              " 'till': 808,\n",
              " 'track': 809,\n",
              " '100': 810,\n",
              " 'green': 811,\n",
              " 'claim': 812,\n",
              " 'medium': 813,\n",
              " 'muslims': 814,\n",
              " 'mount': 815,\n",
              " 'driving': 816,\n",
              " 'miss': 817,\n",
              " 'germ': 818,\n",
              " 'lady': 819,\n",
              " \"ain't\": 820,\n",
              " 'info': 821,\n",
              " 'coach': 822,\n",
              " 'mark': 823,\n",
              " 'room': 824,\n",
              " 'russian': 825,\n",
              " 'thursday': 826,\n",
              " '#gbbo': 827,\n",
              " 'downtown': 828,\n",
              " 'mph': 829,\n",
              " 'british': 830,\n",
              " 'instead': 831,\n",
              " 'wonder': 832,\n",
              " 'isis': 833,\n",
              " 'quiz': 834,\n",
              " \"reddit's\": 835,\n",
              " 'stretcher': 836,\n",
              " 'virgin': 837,\n",
              " 'bestnaijamade': 838,\n",
              " 'bring': 839,\n",
              " 'season': 840,\n",
              " 'taking': 841,\n",
              " 'arsonist': 842,\n",
              " 'scene': 843,\n",
              " 'cry': 844,\n",
              " 'global': 845,\n",
              " 'behind': 846,\n",
              " 'begin': 847,\n",
              " 'job': 848,\n",
              " 'four': 849,\n",
              " 'reuters': 850,\n",
              " 'following': 851,\n",
              " 'scared': 852,\n",
              " 'beat': 853,\n",
              " 'escape': 854,\n",
              " 'burn': 855,\n",
              " 'true': 856,\n",
              " 'theater': 857,\n",
              " 'gave': 858,\n",
              " 'act': 859,\n",
              " 'added': 860,\n",
              " 'climate': 861,\n",
              " 'threat': 862,\n",
              " 'thousand': 863,\n",
              " 'party': 864,\n",
              " 'break': 865,\n",
              " 'wall': 866,\n",
              " 'strike': 867,\n",
              " '0': 868,\n",
              " 'drake': 869,\n",
              " 'bags': 870,\n",
              " 'middle': 871,\n",
              " 'outrage': 872,\n",
              " 'ppl': 873,\n",
              " 'landing': 874,\n",
              " 'york': 875,\n",
              " 'patience': 876,\n",
              " 'former': 877,\n",
              " 'madhya': 878,\n",
              " 'pradesh': 879,\n",
              " 'passenger': 880,\n",
              " 'led': 881,\n",
              " 'lamp': 882,\n",
              " 'gems': 883,\n",
              " 'funtenna': 884,\n",
              " 'ancient': 885,\n",
              " 'subreddits': 886,\n",
              " 'sky': 887,\n",
              " 'awesome': 888,\n",
              " 'ave': 889,\n",
              " 'upon': 890,\n",
              " 'secret': 891,\n",
              " '24': 892,\n",
              " 'reported': 893,\n",
              " '8/': 894,\n",
              " 'turned': 895,\n",
              " 'seeing': 896,\n",
              " 'thinking': 897,\n",
              " 'mode': 898,\n",
              " 'close': 899,\n",
              " 'early': 900,\n",
              " 'pakistani': 901,\n",
              " 'radio': 902,\n",
              " 'question': 903,\n",
              " 'dad': 904,\n",
              " 'bed': 905,\n",
              " 'series': 906,\n",
              " 'working': 907,\n",
              " 'date': 908,\n",
              " 'ca': 909,\n",
              " 'lmao': 910,\n",
              " 'shift': 911,\n",
              " 'civilian': 912,\n",
              " 'pamela': 913,\n",
              " 'answer': 914,\n",
              " 'piece': 915,\n",
              " 'young': 916,\n",
              " 'cut': 917,\n",
              " 'enough': 918,\n",
              " 'dude': 919,\n",
              " 'foot': 920,\n",
              " 'told': 921,\n",
              " 'buy': 922,\n",
              " 'biggest': 923,\n",
              " 'sad': 924,\n",
              " 'firefighter': 925,\n",
              " 'pray': 926,\n",
              " '70th': 927,\n",
              " 'holding': 928,\n",
              " 'likely': 929,\n",
              " 'desire': 930,\n",
              " 'lives': 931,\n",
              " 'course': 932,\n",
              " 'cost': 933,\n",
              " 'sea': 934,\n",
              " '13': 935,\n",
              " 'download': 936,\n",
              " 'years': 937,\n",
              " 'sick': 938,\n",
              " 'bayelsa': 939,\n",
              " 'hailstorm': 940,\n",
              " 'nigerian': 941,\n",
              " 'parole': 942,\n",
              " 'unconfirmed': 943,\n",
              " \"neighbour's\": 944,\n",
              " 'mayhem': 945,\n",
              " \"china's\": 946,\n",
              " 'rly': 947,\n",
              " 'galactic': 948,\n",
              " 'chile': 949,\n",
              " 'expected': 950,\n",
              " 'direction': 951,\n",
              " 'lake': 952,\n",
              " 'london': 953,\n",
              " 'office': 954,\n",
              " 'student': 955,\n",
              " 'teen': 956,\n",
              " 'guess': 957,\n",
              " 'tried': 958,\n",
              " 'wednesday': 959,\n",
              " 'petition': 960,\n",
              " '17': 961,\n",
              " 'career': 962,\n",
              " 'israel': 963,\n",
              " 'match': 964,\n",
              " 'horse': 965,\n",
              " 'started': 966,\n",
              " 'door': 967,\n",
              " 'department': 968,\n",
              " 'rule': 969,\n",
              " 'earth': 970,\n",
              " 'arrested': 971,\n",
              " 'drink': 972,\n",
              " 'worry': 973,\n",
              " 'lie': 974,\n",
              " 'waving': 975,\n",
              " 'geller': 976,\n",
              " 'album': 977,\n",
              " 'gas': 978,\n",
              " 'king': 979,\n",
              " 'australia': 980,\n",
              " 'research': 981,\n",
              " 'f': 982,\n",
              " 'ahead': 983,\n",
              " 'test': 984,\n",
              " 'event': 985,\n",
              " 'silver': 986,\n",
              " 'follow': 987,\n",
              " 'falling': 988,\n",
              " 'standard': 989,\n",
              " 'super': 990,\n",
              " 'ladies': 991,\n",
              " 'tote': 992,\n",
              " 'investigating': 993,\n",
              " 'washington': 994,\n",
              " 'coast': 995,\n",
              " 'ground': 996,\n",
              " 'nagasaki': 997,\n",
              " ...}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37doR9qeQVl5"
      },
      "outputs": [],
      "source": [
        "def text_to_id(text, vocab):\n",
        "    return [vocab.get(token, vocab['<oov>']) for token in text.split()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_luz-TdaQVl5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uODFNR0QVl5"
      },
      "outputs": [],
      "source": [
        "class TweetDisasterDataset(Dataset):\n",
        "    def __init__(self, df, vocab):\n",
        "        self.texts = list(df['filtered_text_joined'].values)\n",
        "        self.labels = list(df['target'].values)\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = torch.tensor(text_to_id(self.texts[idx], self.vocab), dtype=torch.long)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return sequence, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZMPzrDYQVl5"
      },
      "outputs": [],
      "source": [
        "train_dataset = TweetDisasterDataset(pd.concat([X_train, y_train], axis=1), vocab)\n",
        "val_dataset = TweetDisasterDataset(pd.concat([X_val, y_val], axis=1), vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xzEudjlQVl5"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    sequences, labels = zip(*batch)\n",
        "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=vocab['<pad>'])\n",
        "    labels = torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "    return sequences_padded, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faG7T8yGQVl6"
      },
      "outputs": [],
      "source": [
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQQz-dRMQVl6"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r32aSZelUaMD"
      },
      "outputs": [],
      "source": [
        "class TweetDisasterTransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, nhead, num_layers, dropout):\n",
        "        super(TweetDisasterTransformerClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(embedding_dim, 1)\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        emb = emb.transpose(0, 1)\n",
        "        out = self.transformer_encoder(emb)\n",
        "        out = out.mean(dim=0)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "        return torch.sigmoid(out).squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neqzGhMOUaJ9"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 128\n",
        "nhead = 4\n",
        "num_layers_transformer = 2\n",
        "dropout_p = 0.3\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK7zDlq2UaIC",
        "outputId": "fb5909ad-dbc3-4320-d184-84d739ce357e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = TweetDisasterTransformerClassifier(vocab_size, embedding_dim, nhead, num_layers_transformer, dropout_p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5DanzTJUaF-",
        "outputId": "3a7d88aa-d772-410b-92de-74461662921c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO7hTWyiUaEL"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIxm7UvhUaCJ"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRa-5TLEUaAH",
        "outputId": "06b21ccb-72a3-4d2c-bc7b-28f1c58705cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1/25: train Loss: 0.6820, val loss: 0.6650\n",
            "model saved.\n",
            "epoch 2/25: train Loss: 0.6457, val loss: 0.6554\n",
            "model saved.\n",
            "epoch 3/25: train Loss: 0.6171, val loss: 0.6236\n",
            "model saved.\n",
            "epoch 4/25: train Loss: 0.5999, val loss: 0.6085\n",
            "model saved.\n",
            "epoch 5/25: train Loss: 0.5660, val loss: 0.5998\n",
            "model saved.\n",
            "epoch 6/25: train Loss: 0.5512, val loss: 0.5820\n",
            "model saved.\n",
            "epoch 7/25: train Loss: 0.5093, val loss: 0.5911\n",
            "epoch 8/25: train Loss: 0.4853, val loss: 0.5752\n",
            "model saved.\n",
            "epoch 9/25: train Loss: 0.4516, val loss: 0.5797\n",
            "epoch 10/25: train Loss: 0.4107, val loss: 0.5791\n",
            "epoch 11/25: train Loss: 0.3791, val loss: 0.6200\n",
            "early stopping triggered.\n"
          ]
        }
      ],
      "source": [
        "best_transformer_val_loss = np.inf\n",
        "max_epochs_early_stopping = 3\n",
        "counter_early_stopping = 0\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for sequences, labels in train_dataloader:\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(sequences)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "    avg_train_loss = np.mean(train_losses)\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in val_dataloader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            outputs = model(sequences)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            val_losses.append(loss.item())\n",
        "    avg_val_loss = np.mean(val_losses)\n",
        "    print(f\"epoch {epoch+1}/{num_epochs}: train Loss: {avg_train_loss:.4f}, val loss: {avg_val_loss:.4f}\")\n",
        "    if avg_val_loss < best_transformer_val_loss:\n",
        "        best_transformer_val_loss = avg_val_loss\n",
        "        counter_early_stopping = 0\n",
        "        torch.save(model.state_dict(), \"best_transformer_model.pt\")\n",
        "        print(\"model saved.\")\n",
        "    else:\n",
        "        counter_early_stopping += 1\n",
        "        if counter_early_stopping >= max_epochs_early_stopping:\n",
        "            print(\"early stopping triggered.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWRF01gRUZ-K",
        "outputId": "d8420e39-1e03-48fd-9442-2d44a9a4da2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TweetDisasterTransformerClassifier(\n",
              "  (embedding): Embedding(20000, 128, padding_idx=0)\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-1): 2 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"best_transformer_model.pt\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w52c7U3Vl4O"
      },
      "outputs": [],
      "source": [
        "val_preds = []\n",
        "val_labels = []\n",
        "with torch.no_grad():\n",
        "    for sequences, labels in val_dataloader:\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "        outputs = model(sequences)\n",
        "        preds = (outputs > 0.5).int().cpu().numpy()\n",
        "        val_preds.extend(preds)\n",
        "        val_labels.extend(labels.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba4_D9QDVoYB"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yHeAEkpUZ78",
        "outputId": "11797e78-280b-4f02-fc9c-83458dd7f50e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val acc: 0.7321076822061721\n",
            "classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.78      0.77       874\n",
            "         1.0       0.69      0.67      0.68       649\n",
            "\n",
            "    accuracy                           0.73      1523\n",
            "   macro avg       0.73      0.72      0.73      1523\n",
            "weighted avg       0.73      0.73      0.73      1523\n",
            "\n"
          ]
        }
      ],
      "source": [
        "accuracy = accuracy_score(val_labels, val_preds)\n",
        "print(\"val acc:\", accuracy)\n",
        "print(\"classification report:\")\n",
        "print(classification_report(val_labels, val_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ustv7Kp4UZ6H"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(embedding_dim, nhead, num_layers, dropout_rate, num_epochs=10, patience=3):\n",
        "    model = TweetDisasterTransformerClassifier(vocab_size, embedding_dim, nhead, num_layers, dropout_rate)\n",
        "    model.to(device)\n",
        "    loss_fn = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    best_val_loss = np.inf\n",
        "    counter = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for sequences, labels in train_dataloader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sequences)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for sequences, labels in val_dataloader:\n",
        "                sequences, labels = sequences.to(device), labels.to(device)\n",
        "                outputs = model(sequences)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "                val_losses.append(loss.item())\n",
        "        avg_val_loss = np.mean(val_losses)\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            counter = 0\n",
        "            best_model_state = model.state_dict()\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                break\n",
        "    model.load_state_dict(best_model_state)\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in val_dataloader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            outputs = model(sequences)\n",
        "            preds = (outputs > 0.5).int().cpu().numpy()\n",
        "            val_preds.extend(preds)\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "    f1_val_score = f1_score(val_labels, val_preds)\n",
        "    return best_val_loss, f1_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn8XulMoUZ4K"
      },
      "outputs": [],
      "source": [
        "embedding_dims = [64, 128, 256]\n",
        "nheads = [2, 4, 8]\n",
        "num_layers_list = [1, 2, 4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk_hk2hAUZ2H",
        "outputId": "e40786ef-5567-40fe-b855-93c58da38a1d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 64, nhead: 2, num_layers: 1, val_loss: 0.6272, val_f1: 0.6094\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 64, nhead: 2, num_layers: 2, val_loss: 0.5820, val_f1: 0.6708\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 64, nhead: 2, num_layers: 4, val_loss: 0.6005, val_f1: 0.6604\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 64, nhead: 4, num_layers: 1, val_loss: 0.6220, val_f1: 0.5687\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 64, nhead: 4, num_layers: 2, val_loss: 0.6277, val_f1: 0.6347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 64, nhead: 4, num_layers: 4, val_loss: 0.5548, val_f1: 0.6962\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 64, nhead: 8, num_layers: 1, val_loss: 0.6093, val_f1: 0.5952\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 64, nhead: 8, num_layers: 2, val_loss: 0.6033, val_f1: 0.6517\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 64, nhead: 8, num_layers: 4, val_loss: 0.5934, val_f1: 0.6581\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 128, nhead: 2, num_layers: 1, val_loss: 0.6138, val_f1: 0.6548\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 128, nhead: 2, num_layers: 2, val_loss: 0.5837, val_f1: 0.6844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 128, nhead: 2, num_layers: 4, val_loss: 0.5337, val_f1: 0.7128\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 128, nhead: 4, num_layers: 1, val_loss: 0.6052, val_f1: 0.6543\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 128, nhead: 4, num_layers: 2, val_loss: 0.5621, val_f1: 0.7052\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 128, nhead: 4, num_layers: 4, val_loss: 0.5753, val_f1: 0.6698\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 128, nhead: 8, num_layers: 1, val_loss: 0.5843, val_f1: 0.6786\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 128, nhead: 8, num_layers: 2, val_loss: 0.5470, val_f1: 0.6992\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 128, nhead: 8, num_layers: 4, val_loss: 0.5902, val_f1: 0.6322\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 256, nhead: 2, num_layers: 1, val_loss: 0.5930, val_f1: 0.6793\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 256, nhead: 2, num_layers: 2, val_loss: 0.6341, val_f1: 0.6352\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 256, nhead: 2, num_layers: 4, val_loss: 0.5908, val_f1: 0.6918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 256, nhead: 4, num_layers: 1, val_loss: 0.6231, val_f1: 0.6137\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 256, nhead: 4, num_layers: 2, val_loss: 0.5596, val_f1: 0.7089\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 256, nhead: 4, num_layers: 4, val_loss: 0.5546, val_f1: 0.6908\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 256, nhead: 8, num_layers: 1, val_loss: 0.5980, val_f1: 0.6579\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 256, nhead: 8, num_layers: 2, val_loss: 0.5768, val_f1: 0.6977\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_dim: 256, nhead: 8, num_layers: 4, val_loss: 0.5566, val_f1: 0.6232\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "for emb_dim in embedding_dims:\n",
        "    for head in nheads:\n",
        "        for n_layers in num_layers_list:\n",
        "            loss_val, f1_val = train_and_evaluate(emb_dim, head, n_layers, dropout_p, num_epochs=15)\n",
        "            results[(emb_dim, head, n_layers)] = (loss_val, f1_val)\n",
        "            print(f\"embedding_dim: {emb_dim}, nhead: {head}, num_layers: {n_layers}, val_loss: {loss_val:.4f}, val_f1: {f1_val:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tCCE9j7UZ0R",
        "outputId": "eb5a3c9d-e54a-45fc-b683-2a3d8a6eb1b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best hyperparameters: (128, 2, 4) with val_loss: 0.5336895485719045 and f1_val: 0.7128082736674622\n"
          ]
        }
      ],
      "source": [
        "best_params = max(results, key=lambda k: results[k][1])\n",
        "print(\"best hyperparameters:\", best_params, \"with val_loss:\", results[best_params][0], \"and f1_val:\", results[best_params][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o93JH5J7UZyR"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 128\n",
        "nhead = 2\n",
        "num_layers = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF5DOKZrUZwW",
        "outputId": "046669dd-8919-44c2-ced0-afb0fab62633"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TweetDisasterTransformerClassifier(\n",
              "  (embedding): Embedding(20000, 128, padding_idx=0)\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = TweetDisasterTransformerClassifier(vocab_size, embedding_dim, nhead, num_layers, dropout_p)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbUhj2ssUZua"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "best_val_loss = np.inf\n",
        "counter = 0\n",
        "patience = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co3O8sh9UZsc",
        "outputId": "75e253bd-5918-4b24-f03f-11726567fae4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TweetDisasterTransformerClassifier(\n",
              "  (embedding): Embedding(20000, 128, padding_idx=0)\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for sequences, labels in train_dataloader:\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(sequences)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "    avg_train_loss = np.mean(train_losses)\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in val_dataloader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            outputs = model(sequences)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            val_losses.append(loss.item())\n",
        "    avg_val_loss = np.mean(val_losses)\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0\n",
        "        best_model_state = model.state_dict()\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            break\n",
        "model.load_state_dict(best_model_state)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myo-KZ2_WjSw"
      },
      "outputs": [],
      "source": [
        "tknzr = TweetTokenizer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7585twWNWjQx"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_lemmatize(text):\n",
        "    tokens = tknzr.tokenize(text)\n",
        "    return list(map(lemmatizer.lemmatize, tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guzn_H3DWjO0"
      },
      "outputs": [],
      "source": [
        "tokenizer = tokenize_and_lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nnu0BdPAWjMv"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv('test_data.csv')[['text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ws_P5WGsWjK3"
      },
      "outputs": [],
      "source": [
        "test_data['tokenized_text'] = test_data['text'].apply(lambda sent: tokenizer(sent))\n",
        "test_data['filtered_text'] = test_data['tokenized_text'].apply(lambda tokens: [token.lower() for token in tokens if token.lower() not in noise])\n",
        "test_data['filtered_text_joined'] = test_data['filtered_text'].apply(lambda tokens: \" \".join(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km1nF-s5WjI4"
      },
      "outputs": [],
      "source": [
        "test_sequences = [torch.tensor(text_to_id(text, vocab), dtype=torch.long) for text in test_data['filtered_text_joined']]\n",
        "test_sequences_padded = pad_sequence(test_sequences, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
        "test_sequences_padded = test_sequences_padded.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g261UM5hWjGx"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(test_sequences_padded)\n",
        "    test_predictions = (test_outputs > 0.5).int().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGWZxd1GWjEx"
      },
      "outputs": [],
      "source": [
        "sample_submission = pd.read_csv('sample_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nwq_xcMnWjCr"
      },
      "outputs": [],
      "source": [
        "test_submission_transformer = pd.DataFrame(test_predictions, index=sample_submission.id, columns=['target'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqHGz171WjAz"
      },
      "outputs": [],
      "source": [
        "test_submission_transformer.index.name = 'id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukharuaSWi_d"
      },
      "outputs": [],
      "source": [
        "test_submission_transformer.to_csv('test_submission_transformer.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW6_Qbi-6Ny6"
      },
      "source": [
        "Quality of the classification for different sets of hyperparameters:\n",
        "\n",
        "| embedding dim | nhead | num_layers | val Loss | val f1 score  |\n",
        "|---------------|-------|------------|----------|---------|\n",
        "| 64            | 2     | 1          | 0.6272   | 0.6094  |\n",
        "| 64            | 2     | 2          | 0.5820   | 0.6708  |\n",
        "| 64            | 2     | 4          | 0.6005   | 0.6604  |\n",
        "| 64            | 4     | 1          | 0.6220   | 0.5687  |\n",
        "| 64            | 4     | 2          | 0.6277   | 0.6347  |\n",
        "| 64            | 4     | 4          | 0.5548   | 0.6962  |\n",
        "| 64            | 8     | 1          | 0.6093   | 0.5952  |\n",
        "| 64            | 8     | 2          | 0.6033   | 0.6517  |\n",
        "| 64            | 8     | 4          | 0.5934   | 0.6581  |\n",
        "| 128           | 2     | 1          | 0.6138   | 0.6548  |\n",
        "| 128           | 2     | 2          | 0.5837   | 0.6844  |\n",
        "| 128           | 2     | 4          | 0.5337   | **0.7128**  |\n",
        "| 128           | 4     | 1          | 0.6052   | 0.6543  |\n",
        "| 128           | 4     | 2          | 0.5621   | 0.7052  |\n",
        "| 128           | 4     | 4          | 0.5753   | 0.6698  |\n",
        "| 128           | 8     | 1          | 0.5843   | 0.6786  |\n",
        "| 128           | 8     | 2          | 0.5470   | 0.6992  |\n",
        "| 128           | 8     | 4          | 0.5902   | 0.6322  |\n",
        "| 256           | 2     | 1          | 0.5930   | 0.6793  |\n",
        "| 256           | 2     | 2          | 0.6341   | 0.6352  |\n",
        "| 256           | 2     | 4          | 0.5908   | 0.6918  |\n",
        "| 256           | 4     | 1          | 0.6231   | 0.6137  |\n",
        "| 256           | 4     | 2          | 0.5596   | 0.7089  |\n",
        "| 256           | 4     | 4          | 0.5546   | 0.6908  |\n",
        "| 256           | 8     | 1          | 0.5980   | 0.6579  |\n",
        "| 256           | 8     | 2          | 0.5768   | 0.6977  |\n",
        "| 256           | 8     | 4          | 0.5566   | 0.6232  |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1arGPX1pXb9B"
      },
      "source": [
        "public score on the kaggle test dataset: 0.71897"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1eLmQljUTYr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
