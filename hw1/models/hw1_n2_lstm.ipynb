{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Description of the notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the current notebook I've conducted experiments of applying **LSTM** model trained from scratch for classification task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing steps:\n",
        "\n",
        "* tokenization using TweetTokenizer\n",
        "\n",
        "* lemmatization using WordNetLemmatizer\n",
        "\n",
        "* filtering out punctuation symbols and stopwords\n",
        "\n",
        "Exactly that scheme demonstrated the best results on the 1st HW task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters of the LSTM model\n",
        "\n",
        "* embedding dimension: grid = [64, 128, 256] (tests the impact of feature vector size on model expressiveness versus computational cost)\n",
        "\n",
        "* dimension of hidden representation in recurrent neural network: grid = [64, 128, 256]\n",
        "\n",
        "* number of layers of our reccurent neural network: grid = [1, 5, 10] (explores model depth to capture complex patterns, with deeper models risking overfitting)\n",
        "\n",
        "* one- and bidirectional represenation options (bidirectional LSTMs use both past and future context, potentially improving performance)\n",
        "\n",
        "* dropout rate = 0.2 (provides regularization to reduce overfitting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LSTM experiments results\n",
        "\n",
        "| is_bidirectional | embedding_dim | hidden_dim | n_layers | val_loss | val_f1  |\n",
        "|------------------|--------------|------------|----------|----------|---------|\n",
        "| True            | 64           | 64         | 1        | 0.5948   | 0.6209  |\n",
        "| True            | 64           | 64         | 5        | 0.5376   | 0.7049  |\n",
        "| True            | 64           | 64         | 10       | 0.5657   | 0.6778  |\n",
        "| True            | 64           | 128        | 1        | 0.5930   | 0.6363  |\n",
        "| True            | 64           | 128        | 5        | 0.5317   | 0.7102  |\n",
        "| True            | 64           | 128        | 10       | 0.5279   | 0.6929  |\n",
        "| True            | 64           | 256        | 1        | 0.5678   | 0.6711  |\n",
        "| True            | 64           | 256        | 5        | 0.5373   | 0.6945  |\n",
        "| True            | 64           | 256        | 10       | 0.5330   | 0.6951  |\n",
        "| True            | 128          | 64         | 1        | 0.5641   | 0.6420  |\n",
        "| True            | 128          | 64         | 5        | 0.5417   | 0.7015  |\n",
        "| True            | 128          | 64         | 10       | 0.5296   | 0.7095  |\n",
        "| True            | 128          | 128        | 1        | 0.5277   | 0.6932  |\n",
        "| True            | 128          | 128        | 5        | 0.5378   | 0.7127  |\n",
        "| True            | 128          | 128        | 10       | 0.5223   | 0.7010  |\n",
        "| True            | 128          | 256        | 1        | 0.5528   | 0.6747  |\n",
        "| True            | 128          | 256        | 5        | 0.5322   | 0.6990  |\n",
        "| True            | 128          | 256        | 10       | 0.5634   | 0.6985  |\n",
        "| True            | 256          | 64         | 1        | 0.5084   | 0.7058  |\n",
        "| True            | 256          | 64         | 5        | 0.5452   | 0.6924  |\n",
        "| True            | 256          | 64         | 10       | 0.5431   | 0.6989  |\n",
        "| True            | 256          | 128        | 1        | 0.5163   | 0.6990  |\n",
        "| True            | 256          | 128        | 5        | 0.5254   | 0.7063  |\n",
        "| True            | 256          | 128        | 10       | 0.5429   | 0.7020  |\n",
        "| True            | 256          | 256        | 1        | 0.5393   | 0.6861  |\n",
        "| True            | 256          | 256        | 5        | 0.5437   | 0.7048  |\n",
        "| True            | 256          | 256        | 10       | 0.5032   | **0.7293**  |\n",
        "| False           | 64           | 64         | 1        | 0.6235   | 0.6442  |\n",
        "| False           | 64           | 64         | 5        | 0.5612   | 0.6639  |\n",
        "| False           | 64           | 64         | 10       | 0.5926   | 0.6511  |\n",
        "| False           | 64           | 128        | 1        | 0.5550   | 0.6845  |\n",
        "| False           | 64           | 128        | 5        | 0.5499   | 0.6678  |\n",
        "| False           | 64           | 128        | 10       | 0.5295   | 0.6959  |\n",
        "| False           | 64           | 256        | 1        | 0.5287   | 0.6867  |\n",
        "| False           | 64           | 256        | 5        | 0.5322   | 0.6776  |\n",
        "| False           | 64           | 256        | 10       | 0.5384   | 0.6786  |\n",
        "| False           | 128          | 64         | 1        | 0.6041   | 0.6628  |\n",
        "| False           | 128          | 64         | 5        | 0.5792   | 0.6787  |\n",
        "| False           | 128          | 64         | 10       | 0.5609   | 0.7026  |\n",
        "| False           | 128          | 128        | 1        | 0.5598   | 0.7038  |\n",
        "| False           | 128          | 128        | 5        | 0.5423   | 0.7026  |\n",
        "| False           | 128          | 128        | 10       | 0.5559   | 0.6944  |\n",
        "| False           | 128          | 256        | 1        | 0.5477   | 0.6981  |\n",
        "| False           | 128          | 256        | 5        | 0.5279   | 0.7123  |\n",
        "| False           | 128          | 256        | 10       | 0.5273   | 0.7142  |\n",
        "| False           | 256          | 64         | 1        | 0.6279   | 0.6513  |\n",
        "| False           | 256          | 64         | 5        | 0.5825   | 0.6852  |\n",
        "| False           | 256          | 64         | 10       | 0.5629   | 0.6921  |\n",
        "| False           | 256          | 128        | 1        | 0.5699   | 0.7061  |\n",
        "| False           | 256          | 128        | 5        | 0.5605   | 0.6947  |\n",
        "| False           | 256          | 128        | 10       | 0.5418   | 0.7067  |\n",
        "| False           | 256          | 256        | 1        | 0.5635   | 0.6986  |\n",
        "| False           | 256          | 256        | 5        | 0.5326   | 0.7038  |\n",
        "| False           | 256          | 256        | 10       | 0.5425   | 0.7061  |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c7X6wUbIQVlz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Kjk4_z_rQVl0"
      },
      "outputs": [],
      "source": [
        "data_full = pd.read_csv('train_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "htFvY2oQQVl0",
        "outputId": "15291755-10fc-45d8-dccc-54ac92799247"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>There's an emergency evacuation happening now ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>15</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
              "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
              "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
              "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
              "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  \n",
              "5       1  \n",
              "6       1  \n",
              "7       1  \n",
              "8       1  \n",
              "9       1  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_full.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xSrQ5xGdQVl1"
      },
      "outputs": [],
      "source": [
        "data = data_full[['text', 'target']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dGcTOuDUQVl1"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TweetTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jUANG7fhQVl1"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qURpXbvZQq6i",
        "outputId": "0fef0e96-8114-43f9-9c6b-b7a5ddc8333a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\yunes\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MtkeA2igQVl2"
      },
      "outputs": [],
      "source": [
        "tknzr = TweetTokenizer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "u1q7bfQRQVl2"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_lemmatize(text):\n",
        "    tokens = tknzr.tokenize(text)\n",
        "    return list(map(lemmatizer.lemmatize, tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJDmGyr0QVl2",
        "outputId": "bde7ab33-f45e-417e-e47e-786192d24a64"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\yunes\\AppData\\Local\\Temp\\ipykernel_8096\\3405180236.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['tokenized_text'] = data['text'].apply(\n"
          ]
        }
      ],
      "source": [
        "data['tokenized_text'] = data['text'].apply(\n",
        "    lambda sent: tokenize_and_lemmatize(sent)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6qy_ORkQVl2",
        "outputId": "4d9fde38-614c-450a-b426-c4afd3b8779f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords', quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8sJbDlyYQVl3"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "AYtPrJV8QVl3"
      },
      "outputs": [],
      "source": [
        "stopwords_set = set(stopwords.words(\"english\"))\n",
        "punctuation_set = set(punctuation)\n",
        "noise = stopwords_set.union(punctuation_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZDuIcw0QVl3",
        "outputId": "a4af9888-dbaf-45b7-bfff-6af5af0bd4b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\yunes\\AppData\\Local\\Temp\\ipykernel_8096\\668513684.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['filtered_text'] = data['tokenized_text'].apply(\n"
          ]
        }
      ],
      "source": [
        "data['filtered_text'] = data['tokenized_text'].apply(\n",
        "    lambda tokens: [token.lower() for token in tokens if token.lower() not in noise]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "krj07mQOQVl3"
      },
      "outputs": [],
      "source": [
        "data['filtered_text_joined'] = data['filtered_text'].apply(lambda tokens: ' '.join(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "5T17gwCDQVl4",
        "outputId": "d9f43580-48ca-42da-9b1b-d76d63fb4d0d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>tokenized_text</th>\n",
              "      <th>filtered_text</th>\n",
              "      <th>filtered_text_joined</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>[Our, Deeds, are, the, Reason, of, this, #eart...</td>\n",
              "      <td>[deeds, reason, #earthquake, may, allah, forgi...</td>\n",
              "      <td>deeds reason #earthquake may allah forgive u</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>[Forest, fire, near, La, Ronge, Sask, ., Canada]</td>\n",
              "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
              "      <td>forest fire near la ronge sask canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[All, resident, asked, to, ', shelter, in, pla...</td>\n",
              "      <td>[resident, asked, shelter, place, notified, of...</td>\n",
              "      <td>resident asked shelter place notified officer ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>[13,000, people, receive, #wildfires, evacuati...</td>\n",
              "      <td>[13,000, people, receive, #wildfires, evacuati...</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[Just, got, sent, this, photo, from, Ruby, #Al...</td>\n",
              "      <td>[got, sent, photo, ruby, #alaska, smoke, #wild...</td>\n",
              "      <td>got sent photo ruby #alaska smoke #wildfires p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7608</th>\n",
              "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
              "      <td>1</td>\n",
              "      <td>[Two, giant, crane, holding, a, bridge, collap...</td>\n",
              "      <td>[two, giant, crane, holding, bridge, collapse,...</td>\n",
              "      <td>two giant crane holding bridge collapse nearby...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7609</th>\n",
              "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
              "      <td>1</td>\n",
              "      <td>[@aria_ahrary, @TheTawniest, The, out, of, con...</td>\n",
              "      <td>[@aria_ahrary, @thetawniest, control, wild, fi...</td>\n",
              "      <td>@aria_ahrary @thetawniest control wild fire ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7610</th>\n",
              "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
              "      <td>1</td>\n",
              "      <td>[M1, ., 94, [, 01:04, UTC, ], ?, 5km, S, of, V...</td>\n",
              "      <td>[m1, 94, 01:04, utc, 5km, volcano, hawaii, htt...</td>\n",
              "      <td>m1 94 01:04 utc 5km volcano hawaii http://t.co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7611</th>\n",
              "      <td>Police investigating after an e-bike collided ...</td>\n",
              "      <td>1</td>\n",
              "      <td>[Police, investigating, after, an, e-bike, col...</td>\n",
              "      <td>[police, investigating, e-bike, collided, car,...</td>\n",
              "      <td>police investigating e-bike collided car littl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7612</th>\n",
              "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
              "      <td>1</td>\n",
              "      <td>[The, Latest, :, More, Homes, Razed, by, North...</td>\n",
              "      <td>[latest, homes, razed, northern, california, w...</td>\n",
              "      <td>latest homes razed northern california wildfir...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7613 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  target  \\\n",
              "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
              "1                Forest fire near La Ronge Sask. Canada       1   \n",
              "2     All residents asked to 'shelter in place' are ...       1   \n",
              "3     13,000 people receive #wildfires evacuation or...       1   \n",
              "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
              "...                                                 ...     ...   \n",
              "7608  Two giant cranes holding a bridge collapse int...       1   \n",
              "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
              "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
              "7611  Police investigating after an e-bike collided ...       1   \n",
              "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
              "\n",
              "                                         tokenized_text  \\\n",
              "0     [Our, Deeds, are, the, Reason, of, this, #eart...   \n",
              "1      [Forest, fire, near, La, Ronge, Sask, ., Canada]   \n",
              "2     [All, resident, asked, to, ', shelter, in, pla...   \n",
              "3     [13,000, people, receive, #wildfires, evacuati...   \n",
              "4     [Just, got, sent, this, photo, from, Ruby, #Al...   \n",
              "...                                                 ...   \n",
              "7608  [Two, giant, crane, holding, a, bridge, collap...   \n",
              "7609  [@aria_ahrary, @TheTawniest, The, out, of, con...   \n",
              "7610  [M1, ., 94, [, 01:04, UTC, ], ?, 5km, S, of, V...   \n",
              "7611  [Police, investigating, after, an, e-bike, col...   \n",
              "7612  [The, Latest, :, More, Homes, Razed, by, North...   \n",
              "\n",
              "                                          filtered_text  \\\n",
              "0     [deeds, reason, #earthquake, may, allah, forgi...   \n",
              "1         [forest, fire, near, la, ronge, sask, canada]   \n",
              "2     [resident, asked, shelter, place, notified, of...   \n",
              "3     [13,000, people, receive, #wildfires, evacuati...   \n",
              "4     [got, sent, photo, ruby, #alaska, smoke, #wild...   \n",
              "...                                                 ...   \n",
              "7608  [two, giant, crane, holding, bridge, collapse,...   \n",
              "7609  [@aria_ahrary, @thetawniest, control, wild, fi...   \n",
              "7610  [m1, 94, 01:04, utc, 5km, volcano, hawaii, htt...   \n",
              "7611  [police, investigating, e-bike, collided, car,...   \n",
              "7612  [latest, homes, razed, northern, california, w...   \n",
              "\n",
              "                                   filtered_text_joined  \n",
              "0          deeds reason #earthquake may allah forgive u  \n",
              "1                 forest fire near la ronge sask canada  \n",
              "2     resident asked shelter place notified officer ...  \n",
              "3     13,000 people receive #wildfires evacuation or...  \n",
              "4     got sent photo ruby #alaska smoke #wildfires p...  \n",
              "...                                                 ...  \n",
              "7608  two giant crane holding bridge collapse nearby...  \n",
              "7609  @aria_ahrary @thetawniest control wild fire ca...  \n",
              "7610  m1 94 01:04 utc 5km volcano hawaii http://t.co...  \n",
              "7611  police investigating e-bike collided car littl...  \n",
              "7612  latest homes razed northern california wildfir...  \n",
              "\n",
              "[7613 rows x 5 columns]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "urUbCByKQVl4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3bLPE5OOQVl4"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(data['filtered_text_joined'], data['target'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7W2WJ4MNQVl4"
      },
      "outputs": [],
      "source": [
        "def build_vocab(texts, max_words=20000):\n",
        "    token_count_dict = {}\n",
        "    for text in texts:\n",
        "        for token in text.split():\n",
        "            if token not in token_count_dict:\n",
        "                token_count_dict[token] = 1\n",
        "            else:\n",
        "                token_count_dict[token] = token_count_dict[token] + 1\n",
        "\n",
        "    tokens_freq_list = list(token_count_dict.items())\n",
        "    tokens_freq_list.sort(key=lambda x: x[1], reverse=True)\n",
        "    sorted_tokens = tokens_freq_list[:max_words - 2]\n",
        "\n",
        "    vocabulary = {\n",
        "        \"<pad>\": 0,\n",
        "        \"<oov>\": 1,\n",
        "    }\n",
        "\n",
        "    for i, (token, count) in enumerate(sorted_tokens):\n",
        "        vocabulary[token] = i\n",
        "\n",
        "    return vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "D4wXrENEQVl4"
      },
      "outputs": [],
      "source": [
        "vocab = build_vocab(data['filtered_text_joined'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dx1rnshQVl5",
        "outputId": "4c5d142d-26fc-4b58-b833-d848f15cb911"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'<pad>': 0,\n",
              " '<oov>': 1,\n",
              " '...': 0,\n",
              " '\\x89': 1,\n",
              " 'wa': 2,\n",
              " 'like': 3,\n",
              " 'û_': 4,\n",
              " 'fire': 5,\n",
              " 'get': 6,\n",
              " 'ha': 7,\n",
              " 'new': 8,\n",
              " 'via': 9,\n",
              " 'one': 10,\n",
              " 'u': 11,\n",
              " 'people': 12,\n",
              " '2': 13,\n",
              " 'video': 14,\n",
              " 'emergency': 15,\n",
              " 'disaster': 16,\n",
              " 'time': 17,\n",
              " 'body': 18,\n",
              " 'police': 19,\n",
              " 'day': 20,\n",
              " 'year': 21,\n",
              " 'would': 22,\n",
              " 'still': 23,\n",
              " 'building': 24,\n",
              " 'say': 25,\n",
              " 'go': 26,\n",
              " 'news': 27,\n",
              " 'home': 28,\n",
              " 'crash': 29,\n",
              " 'storm': 30,\n",
              " 'back': 31,\n",
              " '..': 32,\n",
              " 'burning': 33,\n",
              " 'know': 34,\n",
              " 'suicide': 35,\n",
              " '3': 36,\n",
              " 'got': 37,\n",
              " 'california': 38,\n",
              " 'see': 39,\n",
              " 'man': 40,\n",
              " 'look': 41,\n",
              " 'car': 42,\n",
              " 'first': 43,\n",
              " 'attack': 44,\n",
              " 'life': 45,\n",
              " 'death': 46,\n",
              " 'bomb': 47,\n",
              " 'train': 48,\n",
              " 'going': 49,\n",
              " 'make': 50,\n",
              " 'love': 51,\n",
              " 'family': 52,\n",
              " 'rt': 53,\n",
              " 'two': 54,\n",
              " 'killed': 55,\n",
              " 'world': 56,\n",
              " 'dead': 57,\n",
              " 'flood': 58,\n",
              " 'û': 59,\n",
              " 'accident': 60,\n",
              " 'nuclear': 61,\n",
              " 'today': 62,\n",
              " 'full': 63,\n",
              " 'want': 64,\n",
              " 'war': 65,\n",
              " 'need': 66,\n",
              " 'good': 67,\n",
              " 'think': 68,\n",
              " 'may': 69,\n",
              " \"can't\": 70,\n",
              " 'way': 71,\n",
              " 'pm': 72,\n",
              " 'watch': 73,\n",
              " 'ûªs': 74,\n",
              " 'many': 75,\n",
              " 'last': 76,\n",
              " '@youtube': 77,\n",
              " 'injury': 78,\n",
              " 'take': 79,\n",
              " 'could': 80,\n",
              " 'collapse': 81,\n",
              " 'w': 82,\n",
              " 'let': 83,\n",
              " 'work': 84,\n",
              " '#news': 85,\n",
              " 'help': 86,\n",
              " '5': 87,\n",
              " '4': 88,\n",
              " 'even': 89,\n",
              " 'weapon': 90,\n",
              " 'mass': 91,\n",
              " 'please': 92,\n",
              " 'another': 93,\n",
              " 'right': 94,\n",
              " 'really': 95,\n",
              " '2015': 96,\n",
              " 'army': 97,\n",
              " '1': 98,\n",
              " 'come': 99,\n",
              " 'lol': 100,\n",
              " 'bombing': 101,\n",
              " 'hiroshima': 102,\n",
              " 'black': 103,\n",
              " 'bag': 104,\n",
              " 'school': 105,\n",
              " 'wildfire': 106,\n",
              " 'city': 107,\n",
              " 'fatality': 108,\n",
              " 'thing': 109,\n",
              " 'read': 110,\n",
              " 'fatal': 111,\n",
              " 'mh370': 112,\n",
              " 'forest': 113,\n",
              " 'much': 114,\n",
              " 'northern': 115,\n",
              " 'water': 116,\n",
              " 'plan': 117,\n",
              " 'great': 118,\n",
              " 'feel': 119,\n",
              " 'bomber': 120,\n",
              " 'never': 121,\n",
              " 'obama': 122,\n",
              " 'fear': 123,\n",
              " 'hostage': 124,\n",
              " 'legionnaires': 125,\n",
              " 'damage': 126,\n",
              " 'wreck': 127,\n",
              " 'latest': 128,\n",
              " 'live': 129,\n",
              " 'every': 130,\n",
              " 'kill': 131,\n",
              " 'hit': 132,\n",
              " 'woman': 133,\n",
              " 'old': 134,\n",
              " 'cause': 135,\n",
              " 'stop': 136,\n",
              " 'atomic': 137,\n",
              " 'top': 138,\n",
              " 'wave': 139,\n",
              " 'said': 140,\n",
              " 'getting': 141,\n",
              " 'shit': 142,\n",
              " 'us': 143,\n",
              " 'house': 144,\n",
              " 'service': 145,\n",
              " 'report': 146,\n",
              " 'injured': 147,\n",
              " 'im': 148,\n",
              " 'near': 149,\n",
              " 'ever': 150,\n",
              " 'since': 151,\n",
              " 'hope': 152,\n",
              " 'wind': 153,\n",
              " 'everyone': 154,\n",
              " \"that's\": 155,\n",
              " 'content': 156,\n",
              " 'coming': 157,\n",
              " 'night': 158,\n",
              " 'truck': 159,\n",
              " 'found': 160,\n",
              " 'state': 161,\n",
              " 'casualty': 162,\n",
              " 'thunderstorm': 163,\n",
              " 'evacuation': 164,\n",
              " 'rain': 165,\n",
              " 'area': 166,\n",
              " 'next': 167,\n",
              " 'without': 168,\n",
              " 'movie': 169,\n",
              " 'fall': 170,\n",
              " 'oil': 171,\n",
              " 'end': 172,\n",
              " 'debris': 173,\n",
              " 'girl': 174,\n",
              " 'warning': 175,\n",
              " 'show': 176,\n",
              " 'cross': 177,\n",
              " 'smoke': 178,\n",
              " 'set': 179,\n",
              " 'god': 180,\n",
              " 'game': 181,\n",
              " '÷': 182,\n",
              " 'update': 183,\n",
              " 'face': 184,\n",
              " 'little': 185,\n",
              " 'run': 186,\n",
              " 'ûª': 187,\n",
              " 'weather': 188,\n",
              " 'wounded': 189,\n",
              " 'malaysia': 190,\n",
              " 'head': 191,\n",
              " 'call': 192,\n",
              " 'food': 193,\n",
              " 'wild': 194,\n",
              " 'boy': 195,\n",
              " 'confirmed': 196,\n",
              " 'severe': 197,\n",
              " 'flooding': 198,\n",
              " 'heat': 199,\n",
              " 'always': 200,\n",
              " 'check': 201,\n",
              " 'fucking': 202,\n",
              " 'well': 203,\n",
              " 'military': 204,\n",
              " 'story': 205,\n",
              " 'keep': 206,\n",
              " 'flame': 207,\n",
              " '70': 208,\n",
              " 'bad': 209,\n",
              " 'sound': 210,\n",
              " 'migrant': 211,\n",
              " 'road': 212,\n",
              " 'also': 213,\n",
              " 'bloody': 214,\n",
              " 'murder': 215,\n",
              " 'blood': 216,\n",
              " 'high': 217,\n",
              " 'liked': 218,\n",
              " 'natural': 219,\n",
              " 'sinking': 220,\n",
              " 'spill': 221,\n",
              " 'loud': 222,\n",
              " 'photo': 223,\n",
              " 'gonna': 224,\n",
              " 'bus': 225,\n",
              " 'best': 226,\n",
              " 'screaming': 227,\n",
              " '11': 228,\n",
              " 'fan': 229,\n",
              " 'japan': 230,\n",
              " 'missing': 231,\n",
              " 'post': 232,\n",
              " 'made': 233,\n",
              " 'week': 234,\n",
              " 'ûò': 235,\n",
              " 'explosion': 236,\n",
              " 'air': 237,\n",
              " 'save': 238,\n",
              " 'child': 239,\n",
              " 'outbreak': 240,\n",
              " 'lightning': 241,\n",
              " 'bridge': 242,\n",
              " 'evacuate': 243,\n",
              " 'failure': 244,\n",
              " 'trapped': 245,\n",
              " 'collided': 246,\n",
              " 'summer': 247,\n",
              " 'someone': 248,\n",
              " 'minute': 249,\n",
              " 'guy': 250,\n",
              " 'lot': 251,\n",
              " 'whole': 252,\n",
              " 'survive': 253,\n",
              " 'hail': 254,\n",
              " 'released': 255,\n",
              " 'attacked': 256,\n",
              " 'panic': 257,\n",
              " 'explode': 258,\n",
              " 'derailment': 259,\n",
              " 'harm': 260,\n",
              " 'rescue': 261,\n",
              " 'thunder': 262,\n",
              " 'reddit': 263,\n",
              " 'wreckage': 264,\n",
              " 'around': 265,\n",
              " 'tonight': 266,\n",
              " '6': 267,\n",
              " 'ambulance': 268,\n",
              " 'sign': 269,\n",
              " 'part': 270,\n",
              " 'destroyed': 271,\n",
              " 'fuck': 272,\n",
              " 'big': 273,\n",
              " 'terrorist': 274,\n",
              " 'hurricane': 275,\n",
              " 'siren': 276,\n",
              " 'hazard': 277,\n",
              " 'burned': 278,\n",
              " 'free': 279,\n",
              " 'island': 280,\n",
              " 'destroy': 281,\n",
              " 'hour': 282,\n",
              " 'friend': 283,\n",
              " 'charged': 284,\n",
              " 'ruin': 285,\n",
              " 'rescued': 286,\n",
              " 'sinkhole': 287,\n",
              " 'county': 288,\n",
              " \"there's\": 289,\n",
              " 'tornado': 290,\n",
              " '15': 291,\n",
              " '\\x9d': 292,\n",
              " 'trauma': 293,\n",
              " 'destruction': 294,\n",
              " 'away': 295,\n",
              " 'white': 296,\n",
              " 'battle': 297,\n",
              " 'put': 298,\n",
              " 'baby': 299,\n",
              " 'boat': 300,\n",
              " 'crush': 301,\n",
              " 'dust': 302,\n",
              " 'survived': 303,\n",
              " 'twister': 304,\n",
              " 'wrecked': 305,\n",
              " 'officer': 306,\n",
              " 'order': 307,\n",
              " 'heart': 308,\n",
              " 'issue': 309,\n",
              " 'p': 310,\n",
              " 'suspect': 311,\n",
              " 'crashed': 312,\n",
              " 'phone': 313,\n",
              " '05': 314,\n",
              " 'light': 315,\n",
              " 'violent': 316,\n",
              " 'real': 317,\n",
              " 'saudi': 318,\n",
              " 'group': 319,\n",
              " 'eye': 320,\n",
              " 'word': 321,\n",
              " 'riot': 322,\n",
              " 'cliff': 323,\n",
              " 'curfew': 324,\n",
              " 'investigators': 325,\n",
              " 'massacre': 326,\n",
              " 'quarantine': 327,\n",
              " 'structural': 328,\n",
              " 'sandstorm': 329,\n",
              " 'sunk': 330,\n",
              " 'windstorm': 331,\n",
              " 'better': 332,\n",
              " 'least': 333,\n",
              " 'market': 334,\n",
              " 'kid': 335,\n",
              " '40': 336,\n",
              " 'came': 337,\n",
              " 'plane': 338,\n",
              " 'national': 339,\n",
              " 'august': 340,\n",
              " 'red': 341,\n",
              " 'hot': 342,\n",
              " 'start': 343,\n",
              " 'saw': 344,\n",
              " 'twitter': 345,\n",
              " 'power': 346,\n",
              " 'change': 347,\n",
              " 'land': 348,\n",
              " 'displaced': 349,\n",
              " 'catastrophe': 350,\n",
              " 'hazardous': 351,\n",
              " 'collapsed': 352,\n",
              " 'collision': 353,\n",
              " 'danger': 354,\n",
              " 'stock': 355,\n",
              " 'devastation': 356,\n",
              " 'drowning': 357,\n",
              " 'engulfed': 358,\n",
              " 'screamed': 359,\n",
              " 'famine': 360,\n",
              " 'bang': 361,\n",
              " 'quarantined': 362,\n",
              " 'past': 363,\n",
              " 'heard': 364,\n",
              " 'thought': 365,\n",
              " 'long': 366,\n",
              " 'mosque': 367,\n",
              " 'wanna': 368,\n",
              " 'oh': 369,\n",
              " 'bleeding': 370,\n",
              " 'tragedy': 371,\n",
              " 'anniversary': 372,\n",
              " 'bombed': 373,\n",
              " 'landslide': 374,\n",
              " 'deluge': 375,\n",
              " 'derail': 376,\n",
              " 'exploded': 377,\n",
              " 'inundated': 378,\n",
              " 'whirlwind': 379,\n",
              " 'use': 380,\n",
              " 'horrible': 381,\n",
              " '9': 382,\n",
              " 'meltdown': 383,\n",
              " 'ok': 384,\n",
              " 'went': 385,\n",
              " 'blast': 386,\n",
              " 'pic': 387,\n",
              " 'blew': 388,\n",
              " 'drown': 389,\n",
              " 'blown': 390,\n",
              " 'rioting': 391,\n",
              " 'traumatised': 392,\n",
              " 'derailed': 393,\n",
              " 'desolation': 394,\n",
              " 'trouble': 395,\n",
              " 'detonate': 396,\n",
              " 'electrocuted': 397,\n",
              " 'rescuers': 398,\n",
              " 'shot': 399,\n",
              " 'something': 400,\n",
              " 'n': 401,\n",
              " 'thank': 402,\n",
              " 'left': 403,\n",
              " 'reunion': 404,\n",
              " 'government': 405,\n",
              " 'fight': 406,\n",
              " 'must': 407,\n",
              " 'zone': 408,\n",
              " 'soon': 409,\n",
              " 'half': 410,\n",
              " 'affected': 411,\n",
              " 'collide': 412,\n",
              " 'typhoon': 413,\n",
              " 'wound': 414,\n",
              " 'earthquake': 415,\n",
              " 'evacuated': 416,\n",
              " 'flattened': 417,\n",
              " 'hijacker': 418,\n",
              " 'lava': 419,\n",
              " 'mudslide': 420,\n",
              " 'panicking': 421,\n",
              " 'street': 422,\n",
              " 'possible': 423,\n",
              " 'airplane': 424,\n",
              " 'official': 425,\n",
              " 'ship': 426,\n",
              " 'iran': 427,\n",
              " 'river': 428,\n",
              " 'apocalypse': 429,\n",
              " 'tomorrow': 430,\n",
              " 'b': 431,\n",
              " '8': 432,\n",
              " 'shoulder': 433,\n",
              " 'stay': 434,\n",
              " 'send': 435,\n",
              " 'drought': 436,\n",
              " 'catastrophic': 437,\n",
              " 'demolish': 438,\n",
              " 'hijacking': 439,\n",
              " 'razed': 440,\n",
              " 'place': 441,\n",
              " 'due': 442,\n",
              " 'second': 443,\n",
              " 'breaking': 444,\n",
              " 'annihilated': 445,\n",
              " 'case': 446,\n",
              " 'survivor': 447,\n",
              " 'arson': 448,\n",
              " 'sure': 449,\n",
              " 'fedex': 450,\n",
              " 'longer': 451,\n",
              " 'blazing': 452,\n",
              " 'song': 453,\n",
              " 'ur': 454,\n",
              " 'bagging': 455,\n",
              " 'pkk': 456,\n",
              " 'caused': 457,\n",
              " 'crushed': 458,\n",
              " '#hot': 459,\n",
              " 'detonation': 460,\n",
              " 'refugee': 461,\n",
              " '2015-08-': 462,\n",
              " 'obliterated': 463,\n",
              " 'pandemonium': 464,\n",
              " 'detonated': 465,\n",
              " 'thanks': 466,\n",
              " 'care': 467,\n",
              " 'used': 468,\n",
              " 'airport': 469,\n",
              " 'book': 470,\n",
              " 'armageddon': 471,\n",
              " 'yet': 472,\n",
              " 'person': 473,\n",
              " 'beautiful': 474,\n",
              " '7': 475,\n",
              " 'bioterror': 476,\n",
              " 'lab': 477,\n",
              " 'nothing': 478,\n",
              " 'murderer': 479,\n",
              " 'chemical': 480,\n",
              " 'drowned': 481,\n",
              " '#prebreak': 482,\n",
              " '#best': 483,\n",
              " 'obliterate': 484,\n",
              " 'three': 485,\n",
              " 'south': 486,\n",
              " 'cool': 487,\n",
              " 'st': 488,\n",
              " 'inside': 489,\n",
              " 'site': 490,\n",
              " 'r': 491,\n",
              " 'tell': 492,\n",
              " 'support': 493,\n",
              " 'shooting': 494,\n",
              " 'already': 495,\n",
              " 'making': 496,\n",
              " 'done': 497,\n",
              " 'play': 498,\n",
              " 'believe': 499,\n",
              " 'ebay': 500,\n",
              " '10': 501,\n",
              " 'remember': 502,\n",
              " 'families': 503,\n",
              " 'calgary': 504,\n",
              " 'security': 505,\n",
              " 'c': 506,\n",
              " 'volcano': 507,\n",
              " 'rise': 508,\n",
              " 'demolished': 509,\n",
              " 'electrocute': 510,\n",
              " 'eyewitness': 511,\n",
              " 'obliteration': 512,\n",
              " 'scream': 513,\n",
              " 'upheaval': 514,\n",
              " 'died': 515,\n",
              " 'far': 516,\n",
              " 'ûó': 517,\n",
              " 'leave': 518,\n",
              " 'traffic': 519,\n",
              " 'actually': 520,\n",
              " 'men': 521,\n",
              " 'ûªt': 522,\n",
              " 'wake': 523,\n",
              " 'move': 524,\n",
              " 'terrorism': 525,\n",
              " 'team': 526,\n",
              " 'policy': 527,\n",
              " 'find': 528,\n",
              " 'turkey': 529,\n",
              " 'bush': 530,\n",
              " 'cyclone': 531,\n",
              " 'demolition': 532,\n",
              " 'hundred': 533,\n",
              " 'tsunami': 534,\n",
              " 'hijack': 535,\n",
              " 'sue': 536,\n",
              " '16yr': 537,\n",
              " 'reason': 538,\n",
              " 'ablaze': 539,\n",
              " 'north': 540,\n",
              " 'mean': 541,\n",
              " '30': 542,\n",
              " 'doe': 543,\n",
              " 'die': 544,\n",
              " 'services': 545,\n",
              " 'nigga': 546,\n",
              " 'country': 547,\n",
              " 'fun': 548,\n",
              " 'bar': 549,\n",
              " 'v': 550,\n",
              " 'peace': 551,\n",
              " 'times': 552,\n",
              " 'israeli': 553,\n",
              " 'trying': 554,\n",
              " 'feeling': 555,\n",
              " 'music': 556,\n",
              " 'deal': 557,\n",
              " 'line': 558,\n",
              " 'effect': 559,\n",
              " 'nearby': 560,\n",
              " 'deluged': 561,\n",
              " 'declares': 562,\n",
              " 'year-old': 563,\n",
              " 'reactor': 564,\n",
              " 'rainstorm': 565,\n",
              " 'snowstorm': 566,\n",
              " 'wait': 567,\n",
              " 'side': 568,\n",
              " 'win': 569,\n",
              " 'brown': 570,\n",
              " 'ago': 571,\n",
              " 'helicopter': 572,\n",
              " 'dog': 573,\n",
              " 'horror': 574,\n",
              " 'anything': 575,\n",
              " 'gun': 576,\n",
              " 'yeah': 577,\n",
              " '50': 578,\n",
              " 'blight': 579,\n",
              " 'mp': 580,\n",
              " 'memory': 581,\n",
              " 'seismic': 582,\n",
              " 'offensive': 583,\n",
              " 'rubble': 584,\n",
              " 'swallowed': 585,\n",
              " 'lost': 586,\n",
              " \"what's\": 587,\n",
              " 'outside': 588,\n",
              " 'west': 589,\n",
              " 'almost': 590,\n",
              " 'hey': 591,\n",
              " 'hell': 592,\n",
              " 'maybe': 593,\n",
              " 'data': 594,\n",
              " 'american': 595,\n",
              " 'action': 596,\n",
              " 'health': 597,\n",
              " 'yes': 598,\n",
              " 'watching': 599,\n",
              " 'bigger': 600,\n",
              " '25': 601,\n",
              " 'typhoon-devastated': 602,\n",
              " 'saipan': 603,\n",
              " 'hat': 604,\n",
              " 'hellfire': 605,\n",
              " 'conclusively': 606,\n",
              " 'center': 607,\n",
              " 'america': 608,\n",
              " 'stand': 609,\n",
              " 'anyone': 610,\n",
              " 'victim': 611,\n",
              " 'bc': 612,\n",
              " 'name': 613,\n",
              " 'hand': 614,\n",
              " '):': 615,\n",
              " 'pick': 616,\n",
              " 'control': 617,\n",
              " 'literally': 618,\n",
              " 'tweet': 619,\n",
              " 'star': 620,\n",
              " 'transport': 621,\n",
              " 'bioterrorism': 622,\n",
              " 'e': 623,\n",
              " 'computer': 624,\n",
              " 'searching': 625,\n",
              " 'level': 626,\n",
              " 'low': 627,\n",
              " 'crew': 628,\n",
              " 'hear': 629,\n",
              " 'desolate': 630,\n",
              " 'utc': 631,\n",
              " 'projected': 632,\n",
              " 'month': 633,\n",
              " '#nowplaying': 634,\n",
              " 'talk': 635,\n",
              " 'vehicle': 636,\n",
              " 'rd': 637,\n",
              " 'mom': 638,\n",
              " 'finally': 639,\n",
              " 'might': 640,\n",
              " 'everything': 641,\n",
              " 'history': 642,\n",
              " 'ball': 643,\n",
              " 'point': 644,\n",
              " 'amid': 645,\n",
              " 'blaze': 646,\n",
              " 'million': 647,\n",
              " 'damn': 648,\n",
              " 'avalanche': 649,\n",
              " 'hollywood': 650,\n",
              " 'pretty': 651,\n",
              " 'online': 652,\n",
              " 'pay': 653,\n",
              " 'though': 654,\n",
              " 'link': 655,\n",
              " 'probably': 656,\n",
              " 'spot': 657,\n",
              " 'saved': 658,\n",
              " 'responder': 659,\n",
              " 'china': 660,\n",
              " 'soudelor': 661,\n",
              " 'aircraft': 662,\n",
              " '#islam': 663,\n",
              " 'manslaughter': 664,\n",
              " 'trench': 665,\n",
              " 'fast': 666,\n",
              " 'try': 667,\n",
              " 'happy': 668,\n",
              " 'feared': 669,\n",
              " 'couple': 670,\n",
              " 'seen': 671,\n",
              " 'share': 672,\n",
              " 'major': 673,\n",
              " 'class': 674,\n",
              " 'crisis': 675,\n",
              " 'tv': 676,\n",
              " 'leather': 677,\n",
              " 'caught': 678,\n",
              " 'town': 679,\n",
              " 'okay': 680,\n",
              " 'youth': 681,\n",
              " 'space': 682,\n",
              " 'problem': 683,\n",
              " 'cake': 684,\n",
              " 'rock': 685,\n",
              " 'money': 686,\n",
              " '#hiroshima': 687,\n",
              " 'tree': 688,\n",
              " 'abc': 689,\n",
              " 'refugio': 690,\n",
              " 'costlier': 691,\n",
              " 'la': 692,\n",
              " '20': 693,\n",
              " 'flash': 694,\n",
              " 'flag': 695,\n",
              " 'huge': 696,\n",
              " 'daily': 697,\n",
              " 'wrong': 698,\n",
              " 'crazy': 699,\n",
              " 'hate': 700,\n",
              " 'annihilation': 701,\n",
              " 'self': 702,\n",
              " 'called': 703,\n",
              " 'blue': 704,\n",
              " 'east': 705,\n",
              " 'business': 706,\n",
              " 'texas': 707,\n",
              " 'nearly': 708,\n",
              " 'morning': 709,\n",
              " 'giant': 710,\n",
              " 'village': 711,\n",
              " '60': 712,\n",
              " 'alarm': 713,\n",
              " 'chance': 714,\n",
              " 'banned': 715,\n",
              " 'knock': 716,\n",
              " 'picking': 717,\n",
              " 'closed': 718,\n",
              " 'across': 719,\n",
              " 'haha': 720,\n",
              " 'lord': 721,\n",
              " 'hard': 722,\n",
              " 'others': 723,\n",
              " 'property': 724,\n",
              " 'drive': 725,\n",
              " 'omg': 726,\n",
              " 'sorry': 727,\n",
              " 'poor': 728,\n",
              " 'toddler': 729,\n",
              " 'united': 730,\n",
              " 'india': 731,\n",
              " 'worst': 732,\n",
              " 'view': 733,\n",
              " 'mishap': 734,\n",
              " 'dont': 735,\n",
              " 'turn': 736,\n",
              " \"let's\": 737,\n",
              " 'entire': 738,\n",
              " 'wow': 739,\n",
              " 'meek': 740,\n",
              " 'aug': 741,\n",
              " '06': 742,\n",
              " 'photos': 743,\n",
              " 'camp': 744,\n",
              " 'become': 745,\n",
              " 'company': 746,\n",
              " 'homes': 747,\n",
              " 'angry': 748,\n",
              " 'ignition': 749,\n",
              " 'devastated': 750,\n",
              " '#earthquake': 751,\n",
              " 'heavy': 752,\n",
              " ':)': 753,\n",
              " 'wanted': 754,\n",
              " 'alone': 755,\n",
              " '16': 756,\n",
              " 'happened': 757,\n",
              " 'aftershock': 758,\n",
              " 'member': 759,\n",
              " '12': 760,\n",
              " 'driver': 761,\n",
              " 'soul': 762,\n",
              " 'film': 763,\n",
              " 'totally': 764,\n",
              " 'give': 765,\n",
              " 'lead': 766,\n",
              " 'learn': 767,\n",
              " 'beach': 768,\n",
              " 'officials': 769,\n",
              " 'force': 770,\n",
              " 'christian': 771,\n",
              " 'temple': 772,\n",
              " 'favorite': 773,\n",
              " 'taken': 774,\n",
              " 'playing': 775,\n",
              " 'anthrax': 776,\n",
              " 'public': 777,\n",
              " 'running': 778,\n",
              " 'looking': 779,\n",
              " 'mad': 780,\n",
              " 'pain': 781,\n",
              " 'open': 782,\n",
              " 'blizzard': 783,\n",
              " 'idea': 784,\n",
              " 'large': 785,\n",
              " 'appears': 786,\n",
              " 'listen': 787,\n",
              " 'centre': 788,\n",
              " 'human': 789,\n",
              " 'issued': 790,\n",
              " 'flight': 791,\n",
              " 'declaration': 792,\n",
              " 'disea': 793,\n",
              " 'front': 794,\n",
              " 'else': 795,\n",
              " 'risk': 796,\n",
              " 'comment': 797,\n",
              " 'moment': 798,\n",
              " 'cop': 799,\n",
              " 'pakistan': 800,\n",
              " 'ûï': 801,\n",
              " 'number': 802,\n",
              " 'trust': 803,\n",
              " 'ready': 804,\n",
              " 'park': 805,\n",
              " 'disease': 806,\n",
              " 'mountain': 807,\n",
              " 'till': 808,\n",
              " 'track': 809,\n",
              " '100': 810,\n",
              " 'green': 811,\n",
              " 'claim': 812,\n",
              " 'medium': 813,\n",
              " 'muslims': 814,\n",
              " 'mount': 815,\n",
              " 'driving': 816,\n",
              " 'miss': 817,\n",
              " 'germ': 818,\n",
              " 'lady': 819,\n",
              " \"ain't\": 820,\n",
              " 'info': 821,\n",
              " 'coach': 822,\n",
              " 'mark': 823,\n",
              " 'room': 824,\n",
              " 'russian': 825,\n",
              " 'thursday': 826,\n",
              " '#gbbo': 827,\n",
              " 'downtown': 828,\n",
              " 'mph': 829,\n",
              " 'british': 830,\n",
              " 'instead': 831,\n",
              " 'wonder': 832,\n",
              " 'isis': 833,\n",
              " 'quiz': 834,\n",
              " \"reddit's\": 835,\n",
              " 'stretcher': 836,\n",
              " 'virgin': 837,\n",
              " 'bestnaijamade': 838,\n",
              " 'bring': 839,\n",
              " 'season': 840,\n",
              " 'taking': 841,\n",
              " 'arsonist': 842,\n",
              " 'scene': 843,\n",
              " 'cry': 844,\n",
              " 'global': 845,\n",
              " 'behind': 846,\n",
              " 'begin': 847,\n",
              " 'job': 848,\n",
              " 'four': 849,\n",
              " 'reuters': 850,\n",
              " 'following': 851,\n",
              " 'scared': 852,\n",
              " 'beat': 853,\n",
              " 'escape': 854,\n",
              " 'burn': 855,\n",
              " 'true': 856,\n",
              " 'theater': 857,\n",
              " 'gave': 858,\n",
              " 'act': 859,\n",
              " 'added': 860,\n",
              " 'climate': 861,\n",
              " 'threat': 862,\n",
              " 'thousand': 863,\n",
              " 'party': 864,\n",
              " 'break': 865,\n",
              " 'wall': 866,\n",
              " 'strike': 867,\n",
              " '0': 868,\n",
              " 'drake': 869,\n",
              " 'bags': 870,\n",
              " 'middle': 871,\n",
              " 'outrage': 872,\n",
              " 'ppl': 873,\n",
              " 'landing': 874,\n",
              " 'york': 875,\n",
              " 'patience': 876,\n",
              " 'former': 877,\n",
              " 'madhya': 878,\n",
              " 'pradesh': 879,\n",
              " 'passenger': 880,\n",
              " 'led': 881,\n",
              " 'lamp': 882,\n",
              " 'gems': 883,\n",
              " 'funtenna': 884,\n",
              " 'ancient': 885,\n",
              " 'subreddits': 886,\n",
              " 'sky': 887,\n",
              " 'awesome': 888,\n",
              " 'ave': 889,\n",
              " 'upon': 890,\n",
              " 'secret': 891,\n",
              " '24': 892,\n",
              " 'reported': 893,\n",
              " '8/': 894,\n",
              " 'turned': 895,\n",
              " 'seeing': 896,\n",
              " 'thinking': 897,\n",
              " 'mode': 898,\n",
              " 'close': 899,\n",
              " 'early': 900,\n",
              " 'pakistani': 901,\n",
              " 'radio': 902,\n",
              " 'question': 903,\n",
              " 'dad': 904,\n",
              " 'bed': 905,\n",
              " 'series': 906,\n",
              " 'working': 907,\n",
              " 'date': 908,\n",
              " 'ca': 909,\n",
              " 'lmao': 910,\n",
              " 'shift': 911,\n",
              " 'civilian': 912,\n",
              " 'pamela': 913,\n",
              " 'answer': 914,\n",
              " 'piece': 915,\n",
              " 'young': 916,\n",
              " 'cut': 917,\n",
              " 'enough': 918,\n",
              " 'dude': 919,\n",
              " 'foot': 920,\n",
              " 'told': 921,\n",
              " 'buy': 922,\n",
              " 'biggest': 923,\n",
              " 'sad': 924,\n",
              " 'firefighter': 925,\n",
              " 'pray': 926,\n",
              " '70th': 927,\n",
              " 'holding': 928,\n",
              " 'likely': 929,\n",
              " 'desire': 930,\n",
              " 'lives': 931,\n",
              " 'course': 932,\n",
              " 'cost': 933,\n",
              " 'sea': 934,\n",
              " '13': 935,\n",
              " 'download': 936,\n",
              " 'years': 937,\n",
              " 'sick': 938,\n",
              " 'bayelsa': 939,\n",
              " 'hailstorm': 940,\n",
              " 'nigerian': 941,\n",
              " 'parole': 942,\n",
              " 'unconfirmed': 943,\n",
              " \"neighbour's\": 944,\n",
              " 'mayhem': 945,\n",
              " \"china's\": 946,\n",
              " 'rly': 947,\n",
              " 'galactic': 948,\n",
              " 'chile': 949,\n",
              " 'expected': 950,\n",
              " 'direction': 951,\n",
              " 'lake': 952,\n",
              " 'london': 953,\n",
              " 'office': 954,\n",
              " 'student': 955,\n",
              " 'teen': 956,\n",
              " 'guess': 957,\n",
              " 'tried': 958,\n",
              " 'wednesday': 959,\n",
              " 'petition': 960,\n",
              " '17': 961,\n",
              " 'career': 962,\n",
              " 'israel': 963,\n",
              " 'match': 964,\n",
              " 'horse': 965,\n",
              " 'started': 966,\n",
              " 'door': 967,\n",
              " 'department': 968,\n",
              " 'rule': 969,\n",
              " 'earth': 970,\n",
              " 'arrested': 971,\n",
              " 'drink': 972,\n",
              " 'worry': 973,\n",
              " 'lie': 974,\n",
              " 'waving': 975,\n",
              " 'geller': 976,\n",
              " 'album': 977,\n",
              " 'gas': 978,\n",
              " 'king': 979,\n",
              " 'australia': 980,\n",
              " 'research': 981,\n",
              " 'f': 982,\n",
              " 'ahead': 983,\n",
              " 'test': 984,\n",
              " 'event': 985,\n",
              " 'silver': 986,\n",
              " 'follow': 987,\n",
              " 'falling': 988,\n",
              " 'standard': 989,\n",
              " 'super': 990,\n",
              " 'ladies': 991,\n",
              " 'tote': 992,\n",
              " 'investigating': 993,\n",
              " 'washington': 994,\n",
              " 'coast': 995,\n",
              " 'ground': 996,\n",
              " 'nagasaki': 997,\n",
              " ...}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "37doR9qeQVl5"
      },
      "outputs": [],
      "source": [
        "def text_to_id(text, vocab):\n",
        "    return [vocab.get(token, vocab['<oov>']) for token in text.split()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_luz-TdaQVl5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6uODFNR0QVl5"
      },
      "outputs": [],
      "source": [
        "class TweetDisasterDataset(Dataset):\n",
        "    def __init__(self, df, vocab):\n",
        "        self.texts = list(df['filtered_text_joined'].values)\n",
        "        self.labels = list(df['target'].values)\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = torch.tensor(text_to_id(self.texts[idx], self.vocab), dtype=torch.long)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return sequence, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1XmIYmzQVl5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "PZMPzrDYQVl5"
      },
      "outputs": [],
      "source": [
        "train_dataset = TweetDisasterDataset(pd.concat([X_train, y_train], axis=1), vocab)\n",
        "val_dataset = TweetDisasterDataset(pd.concat([X_val, y_val], axis=1), vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "4xzEudjlQVl5"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    sequences, labels = zip(*batch)\n",
        "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=vocab['<pad>'])\n",
        "    labels = torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "    return sequences_padded, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "faG7T8yGQVl6"
      },
      "outputs": [],
      "source": [
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "wQQz-dRMQVl6"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lM54Zt5CQVl6"
      },
      "outputs": [],
      "source": [
        "class TweetDisasterLSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, bidirectional=False, num_layers=1, dropout=0.3):\n",
        "        super(TweetDisasterLSTMClassifier, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, bidirectional=bidirectional,\n",
        "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding_vector = self.embedding(x)\n",
        "        out, (hid, c) = self.lstm(embedding_vector)\n",
        "\n",
        "        return torch.sigmoid(self.fc(self.dropout(hid[-1]))).squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "KJfFyzw-QVl6"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 128\n",
        "hidden_dim = 64\n",
        "num_layers = 1\n",
        "dropout_p = 0.3\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "awdJ4wEcQVl6"
      },
      "outputs": [],
      "source": [
        "model = TweetDisasterLSTMClassifier(vocab_size, embedding_dim, hidden_dim, False, num_layers, dropout_p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2AVm6QEQVl6",
        "outputId": "4aef5c67-0eb3-435d-a1c1-aa41aa7424cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "eVLHB_IAQVl7"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Kp4lrnzPQVl7"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "93qkQNP8QVl7"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "owqPgOlxQVl7"
      },
      "outputs": [],
      "source": [
        "best_lstm_val_loss = np.inf\n",
        "max_epochs_early_stopping= 3\n",
        "counter_early_stopping = 0\n",
        "num_epochs = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAi9y9g9QVl7",
        "outputId": "4cc3bb5e-7687-433d-bad6-4e2d5851812a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1/25: train Loss: 0.6903, val loss: 0.6870\n",
            "model saved.\n",
            "epoch 2/25: train Loss: 0.6857, val loss: 0.6824\n",
            "model saved.\n",
            "epoch 3/25: train Loss: 0.6816, val loss: 0.6776\n",
            "model saved.\n",
            "epoch 4/25: train Loss: 0.6766, val loss: 0.6717\n",
            "model saved.\n",
            "epoch 5/25: train Loss: 0.6692, val loss: 0.6636\n",
            "model saved.\n",
            "epoch 6/25: train Loss: 0.6525, val loss: 0.6537\n",
            "model saved.\n",
            "epoch 7/25: train Loss: 0.6199, val loss: 0.6495\n",
            "model saved.\n",
            "epoch 8/25: train Loss: 0.5873, val loss: 0.6455\n",
            "model saved.\n",
            "epoch 9/25: train Loss: 0.5498, val loss: 0.6436\n",
            "model saved.\n",
            "epoch 10/25: train Loss: 0.5052, val loss: 0.6306\n",
            "model saved.\n",
            "epoch 11/25: train Loss: 0.4642, val loss: 0.6200\n",
            "model saved.\n",
            "epoch 12/25: train Loss: 0.4382, val loss: 0.6166\n",
            "model saved.\n",
            "epoch 13/25: train Loss: 0.4062, val loss: 0.6225\n",
            "epoch 14/25: train Loss: 0.3791, val loss: 0.6331\n",
            "epoch 15/25: train Loss: 0.3639, val loss: 0.6329\n",
            "early stopping triggered.\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for sequences, labels in train_dataloader:\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(sequences)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "    avg_train_loss = np.mean(train_losses)\n",
        "\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in val_dataloader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            outputs = model(sequences)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            val_losses.append(loss.item())\n",
        "\n",
        "    avg_val_loss = np.mean(val_losses)\n",
        "    print(f\"epoch {epoch+1}/{num_epochs}: train Loss: {avg_train_loss:.4f}, val loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    if avg_val_loss < best_lstm_val_loss:\n",
        "        best_lstm_val_loss = avg_val_loss\n",
        "        counter_early_stopping = 0\n",
        "        torch.save(model.state_dict(), \"best_lstm_model.pt\")\n",
        "        print(\"model saved.\")\n",
        "    else:\n",
        "        counter_early_stopping += 1\n",
        "        if counter_early_stopping >= max_epochs_early_stopping:\n",
        "            print(\"early stopping triggered.\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIRjIIRtQVl7",
        "outputId": "fe701b16-48f1-4105-d960-4704ad1b9cd3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TweetDisasterLSTMClassifier(\n",
              "  (embedding): Embedding(20000, 128, padding_idx=0)\n",
              "  (lstm): LSTM(128, 64, batch_first=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"best_lstm_model.pt\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpDJYWp9QVl8"
      },
      "outputs": [],
      "source": [
        "val_preds = []\n",
        "val_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sequences, labels in val_dataloader:\n",
        "        sequences = sequences.to(device)\n",
        "        outputs = model(sequences)\n",
        "        preds = (outputs > 0.5).int().cpu().numpy()\n",
        "        val_preds.extend(preds)\n",
        "        val_labels.extend(labels.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N0DiSRWQVl9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnIu34cVQVl9",
        "outputId": "3e6b77ea-58e4-41ba-e099-9431ebc5c7d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val acc: 0.7051871306631649\n",
            "classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.71      0.73       874\n",
            "         1.0       0.64      0.70      0.67       649\n",
            "\n",
            "    accuracy                           0.71      1523\n",
            "   macro avg       0.70      0.70      0.70      1523\n",
            "weighted avg       0.71      0.71      0.71      1523\n",
            "\n"
          ]
        }
      ],
      "source": [
        "accuracy = accuracy_score(val_labels, val_preds)\n",
        "print(\"val acc:\", accuracy)\n",
        "print(\"classification report:\")\n",
        "print(classification_report(val_labels, val_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcWiyL6AQVl9"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(embedding_dim, hidden_dim, bidirectional=False, num_layers=1, dropout_rate=0.2, num_epochs=10, patience=3):\n",
        "    model = TweetDisasterLSTMClassifier(vocab_size, embedding_dim, hidden_dim, bidirectional, num_layers, dropout_p)\n",
        "    model.to(device)\n",
        "    loss_fn = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    best_val_loss = np.inf\n",
        "    counter = 0\n",
        "    patience = 3\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for sequences, labels in train_dataloader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(sequences)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for sequences, labels in val_dataloader:\n",
        "                sequences, labels = sequences.to(device), labels.to(device)\n",
        "                outputs = model(sequences)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "                val_losses.append(loss.item())\n",
        "        avg_val_loss = np.mean(val_losses)\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            counter = 0\n",
        "            best_model_state = model.state_dict()\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                break\n",
        "    model.load_state_dict(best_model_state)\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in val_dataloader:\n",
        "            sequences = sequences.to(device)\n",
        "            outputs = model(sequences)\n",
        "            preds = (outputs > 0.5).int().cpu().numpy()\n",
        "            val_preds.extend(preds)\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "    f1_val_score = f1_score(val_labels, val_preds)\n",
        "    return best_val_loss, f1_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4TSulCHQVl9"
      },
      "outputs": [],
      "source": [
        "bidirectional = [True, False]\n",
        "embedding_dims = [64, 128, 256]\n",
        "hidden_dims = [64, 128, 256]\n",
        "num_layers = [1, 5, 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp67w8gfQVl-",
        "outputId": "f20cf118-e95f-4951-99fb-0df200eb860c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is_bidirectional: True, embedding_dim: 64, hidden_dim: 64, n_layers: 1, val_loss: 0.5948, val_f1: 0.6209\n",
            "is_bidirectional: True, embedding_dim: 64, hidden_dim: 64, n_layers: 5, val_loss: 0.5376, val_f1: 0.7049\n",
            "is_bidirectional: True, embedding_dim: 64, hidden_dim: 64, n_layers: 10, val_loss: 0.5657, val_f1: 0.6778\n",
            "is_bidirectional: True, embedding_dim: 64, hidden_dim: 128, n_layers: 1, val_loss: 0.5930, val_f1: 0.6363\n",
            "is_bidirectional: True, embedding_dim: 64, hidden_dim: 128, n_layers: 5, val_loss: 0.5317, val_f1: 0.7102\n",
            "is_bidirectional: True, embedding_dim: 64, hidden_dim: 128, n_layers: 10, val_loss: 0.5279, val_f1: 0.6929\n",
            "is_bidirectional: True, embedding_dim: 64, hidden_dim: 256, n_layers: 1, val_loss: 0.5678, val_f1: 0.6711\n",
            "is_bidirectional: True, embedding_dim: 64, hidden_dim: 256, n_layers: 5, val_loss: 0.5373, val_f1: 0.6945\n",
            "is_bidirectional: True, embedding_dim: 64, hidden_dim: 256, n_layers: 10, val_loss: 0.5330, val_f1: 0.6951\n",
            "is_bidirectional: True, embedding_dim: 128, hidden_dim: 64, n_layers: 1, val_loss: 0.5641, val_f1: 0.6420\n",
            "is_bidirectional: True, embedding_dim: 128, hidden_dim: 64, n_layers: 5, val_loss: 0.5417, val_f1: 0.7015\n",
            "is_bidirectional: True, embedding_dim: 128, hidden_dim: 64, n_layers: 10, val_loss: 0.5296, val_f1: 0.7095\n",
            "is_bidirectional: True, embedding_dim: 128, hidden_dim: 128, n_layers: 1, val_loss: 0.5277, val_f1: 0.6932\n",
            "is_bidirectional: True, embedding_dim: 128, hidden_dim: 128, n_layers: 5, val_loss: 0.5378, val_f1: 0.7127\n",
            "is_bidirectional: True, embedding_dim: 128, hidden_dim: 128, n_layers: 10, val_loss: 0.5223, val_f1: 0.7010\n",
            "is_bidirectional: True, embedding_dim: 128, hidden_dim: 256, n_layers: 1, val_loss: 0.5528, val_f1: 0.6747\n",
            "is_bidirectional: True, embedding_dim: 128, hidden_dim: 256, n_layers: 5, val_loss: 0.5322, val_f1: 0.6990\n",
            "is_bidirectional: True, embedding_dim: 128, hidden_dim: 256, n_layers: 10, val_loss: 0.5634, val_f1: 0.6985\n",
            "is_bidirectional: True, embedding_dim: 256, hidden_dim: 64, n_layers: 1, val_loss: 0.5084, val_f1: 0.7058\n",
            "is_bidirectional: True, embedding_dim: 256, hidden_dim: 64, n_layers: 5, val_loss: 0.5452, val_f1: 0.6924\n",
            "is_bidirectional: True, embedding_dim: 256, hidden_dim: 64, n_layers: 10, val_loss: 0.5431, val_f1: 0.6989\n",
            "is_bidirectional: True, embedding_dim: 256, hidden_dim: 128, n_layers: 1, val_loss: 0.5163, val_f1: 0.6990\n",
            "is_bidirectional: True, embedding_dim: 256, hidden_dim: 128, n_layers: 5, val_loss: 0.5254, val_f1: 0.7063\n",
            "is_bidirectional: True, embedding_dim: 256, hidden_dim: 128, n_layers: 10, val_loss: 0.5429, val_f1: 0.7020\n",
            "is_bidirectional: True, embedding_dim: 256, hidden_dim: 256, n_layers: 1, val_loss: 0.5393, val_f1: 0.6861\n",
            "is_bidirectional: True, embedding_dim: 256, hidden_dim: 256, n_layers: 5, val_loss: 0.5437, val_f1: 0.7048\n",
            "is_bidirectional: True, embedding_dim: 256, hidden_dim: 256, n_layers: 10, val_loss: 0.5032, val_f1: 0.7293\n",
            "is_bidirectional: False, embedding_dim: 64, hidden_dim: 64, n_layers: 1, val_loss: 0.6235, val_f1: 0.6442\n",
            "is_bidirectional: False, embedding_dim: 64, hidden_dim: 64, n_layers: 5, val_loss: 0.5612, val_f1: 0.6639\n",
            "is_bidirectional: False, embedding_dim: 64, hidden_dim: 64, n_layers: 10, val_loss: 0.5926, val_f1: 0.6511\n",
            "is_bidirectional: False, embedding_dim: 64, hidden_dim: 128, n_layers: 1, val_loss: 0.5550, val_f1: 0.6845\n",
            "is_bidirectional: False, embedding_dim: 64, hidden_dim: 128, n_layers: 5, val_loss: 0.5499, val_f1: 0.6678\n",
            "is_bidirectional: False, embedding_dim: 64, hidden_dim: 128, n_layers: 10, val_loss: 0.5295, val_f1: 0.6959\n",
            "is_bidirectional: False, embedding_dim: 64, hidden_dim: 256, n_layers: 1, val_loss: 0.5287, val_f1: 0.6867\n",
            "is_bidirectional: False, embedding_dim: 64, hidden_dim: 256, n_layers: 5, val_loss: 0.5322, val_f1: 0.6776\n",
            "is_bidirectional: False, embedding_dim: 64, hidden_dim: 256, n_layers: 10, val_loss: 0.5384, val_f1: 0.6786\n",
            "is_bidirectional: False, embedding_dim: 128, hidden_dim: 64, n_layers: 1, val_loss: 0.6041, val_f1: 0.6628\n",
            "is_bidirectional: False, embedding_dim: 128, hidden_dim: 64, n_layers: 5, val_loss: 0.5792, val_f1: 0.6787\n",
            "is_bidirectional: False, embedding_dim: 128, hidden_dim: 64, n_layers: 10, val_loss: 0.5609, val_f1: 0.7026\n",
            "is_bidirectional: False, embedding_dim: 128, hidden_dim: 128, n_layers: 1, val_loss: 0.5598, val_f1: 0.7038\n",
            "is_bidirectional: False, embedding_dim: 128, hidden_dim: 128, n_layers: 5, val_loss: 0.5423, val_f1: 0.7026\n",
            "is_bidirectional: False, embedding_dim: 128, hidden_dim: 128, n_layers: 10, val_loss: 0.5559, val_f1: 0.6944\n",
            "is_bidirectional: False, embedding_dim: 128, hidden_dim: 256, n_layers: 1, val_loss: 0.5477, val_f1: 0.6981\n",
            "is_bidirectional: False, embedding_dim: 128, hidden_dim: 256, n_layers: 5, val_loss: 0.5279, val_f1: 0.7123\n",
            "is_bidirectional: False, embedding_dim: 128, hidden_dim: 256, n_layers: 10, val_loss: 0.5273, val_f1: 0.7142\n",
            "is_bidirectional: False, embedding_dim: 256, hidden_dim: 64, n_layers: 1, val_loss: 0.6279, val_f1: 0.6513\n",
            "is_bidirectional: False, embedding_dim: 256, hidden_dim: 64, n_layers: 5, val_loss: 0.5825, val_f1: 0.6852\n",
            "is_bidirectional: False, embedding_dim: 256, hidden_dim: 64, n_layers: 10, val_loss: 0.5629, val_f1: 0.6921\n",
            "is_bidirectional: False, embedding_dim: 256, hidden_dim: 128, n_layers: 1, val_loss: 0.5699, val_f1: 0.7061\n",
            "is_bidirectional: False, embedding_dim: 256, hidden_dim: 128, n_layers: 5, val_loss: 0.5605, val_f1: 0.6947\n",
            "is_bidirectional: False, embedding_dim: 256, hidden_dim: 128, n_layers: 10, val_loss: 0.5418, val_f1: 0.7067\n",
            "is_bidirectional: False, embedding_dim: 256, hidden_dim: 256, n_layers: 1, val_loss: 0.5635, val_f1: 0.6986\n",
            "is_bidirectional: False, embedding_dim: 256, hidden_dim: 256, n_layers: 5, val_loss: 0.5326, val_f1: 0.7038\n",
            "is_bidirectional: False, embedding_dim: 256, hidden_dim: 256, n_layers: 10, val_loss: 0.5425, val_f1: 0.7061\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "for is_bidirectional in bidirectional:\n",
        "    for emb_dim in embedding_dims:\n",
        "        for hid_dim in hidden_dims:\n",
        "            for n_layers in num_layers:\n",
        "                loss_val, f1_val = train_and_evaluate(emb_dim, hid_dim, is_bidirectional, num_layers=n_layers, num_epochs=15)\n",
        "                results[(is_bidirectional, emb_dim, hid_dim, n_layers)] = (loss_val, f1_val)\n",
        "                print(f\"is_bidirectional: {is_bidirectional}, embedding_dim: {emb_dim}, hidden_dim: {hid_dim}, n_layers: {n_layers}, val_loss: {loss_val:.4f}, val_f1: {f1_val:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dem9F3WlQVl-",
        "outputId": "d25a57f3-b011-46e7-c211-ddb41b359f88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best hyperparameters: (True, 256, 256, 10) with val_loss: 0.503241935124 and f1_val: 0.7292817679558011\n"
          ]
        }
      ],
      "source": [
        "best_params = max(results, key=lambda k: results[k][1])\n",
        "print(\"best hyperparameters:\", best_params, \"with val_loss:\", results[best_params][0], \"and f1_val:\", results[best_params][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "NCswLBJoQVl-"
      },
      "outputs": [],
      "source": [
        "embedding_dim_one_directional = 256\n",
        "hidden_dim_one_directional = 128\n",
        "num_layers_one_directional = 10\n",
        "is_bidirectional = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "bLtePWs1XxD3"
      },
      "outputs": [],
      "source": [
        "# for bidirectional\n",
        "embedding_dim = 256\n",
        "hidden_dim = 256\n",
        "num_layers = 10\n",
        "is_bidirectional = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_QRa0B8QVl-",
        "outputId": "d57acd4f-39c7-4fac-f35e-0b1030fa2bf3"
      },
      "outputs": [],
      "source": [
        "model = TweetDisasterLSTMClassifier(vocab_size, embedding_dim, hidden_dim, is_bidirectional, num_layers, dropout_p)\n",
        "model.to(device)\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "best_val_loss = np.inf\n",
        "counter = 0\n",
        "patience = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for sequences, labels in train_dataloader:\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(sequences)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "    avg_train_loss = np.mean(train_losses)\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in val_dataloader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            outputs = model(sequences)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            val_losses.append(loss.item())\n",
        "    avg_val_loss = np.mean(val_losses)\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0\n",
        "        best_model_state = model.state_dict()\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            break\n",
        "model.load_state_dict(best_model_state)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsRJ2e_JQVl-"
      },
      "outputs": [],
      "source": [
        "tknzr = TweetTokenizer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUHuZ6-kQVl-"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_lemmatize(text):\n",
        "    tokens = tknzr.tokenize(text)\n",
        "    return list(map(lemmatizer.lemmatize, tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vgxvq-BoQVl-"
      },
      "outputs": [],
      "source": [
        "tokenizer = tokenize_and_lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVlyvR6aQVl_"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv('test_data.csv')[['text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDtzOHkmQVl_"
      },
      "outputs": [],
      "source": [
        "test_data['tokenized_text'] = test_data['text'].apply(\n",
        "    lambda sent: tokenizer(sent)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RD1wDsdQVl_"
      },
      "outputs": [],
      "source": [
        "test_data['filtered_text'] = test_data['tokenized_text'].apply(\n",
        "    lambda tokens: [token.lower() for token in tokens if token.lower() not in noise]\n",
        ")\n",
        "\n",
        "test_data['filtered_text_joined'] = test_data['filtered_text'].apply(lambda tokens: \" \".join(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTQttzYbQVl_"
      },
      "outputs": [],
      "source": [
        "test_sequences = [torch.tensor(text_to_id(text, vocab), dtype=torch.long) for text in test_data['filtered_text_joined']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOxT6juvTIig"
      },
      "outputs": [],
      "source": [
        "test_sequences_padded = pad_sequence(test_sequences, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
        "test_sequences_padded = test_sequences_padded.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-deoalI4TIgk"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(test_sequences_padded)\n",
        "    test_predictions = (test_outputs > 0.5).int().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QZChdT6TIev",
        "outputId": "b870d3b2-8072-4f9e-d31b-d067f5c7e420"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 0, 1, ..., 1, 1, 1], dtype=int32)"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjEuwrHtTIcz"
      },
      "outputs": [],
      "source": [
        "sample_submission = pd.read_csv('sample_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfcwWK0hTIas"
      },
      "outputs": [],
      "source": [
        "test_submission_lstm = pd.DataFrame(test_predictions, index=sample_submission.id, columns=['target'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5fY8mIbTIYa"
      },
      "outputs": [],
      "source": [
        "test_submission_lstm.index.name = 'id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQr7xqr9TIWe"
      },
      "outputs": [],
      "source": [
        "test_submission_lstm.to_csv('test_submission_lstm_bidirectional.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcoGe8KSc_ha"
      },
      "source": [
        "LSTM experiments results\n",
        "\n",
        "| is_bidirectional | embedding_dim | hidden_dim | n_layers | val_loss | val_f1  |\n",
        "|------------------|--------------|------------|----------|----------|---------|\n",
        "| True            | 64           | 64         | 1        | 0.5948   | 0.6209  |\n",
        "| True            | 64           | 64         | 5        | 0.5376   | 0.7049  |\n",
        "| True            | 64           | 64         | 10       | 0.5657   | 0.6778  |\n",
        "| True            | 64           | 128        | 1        | 0.5930   | 0.6363  |\n",
        "| True            | 64           | 128        | 5        | 0.5317   | 0.7102  |\n",
        "| True            | 64           | 128        | 10       | 0.5279   | 0.6929  |\n",
        "| True            | 64           | 256        | 1        | 0.5678   | 0.6711  |\n",
        "| True            | 64           | 256        | 5        | 0.5373   | 0.6945  |\n",
        "| True            | 64           | 256        | 10       | 0.5330   | 0.6951  |\n",
        "| True            | 128          | 64         | 1        | 0.5641   | 0.6420  |\n",
        "| True            | 128          | 64         | 5        | 0.5417   | 0.7015  |\n",
        "| True            | 128          | 64         | 10       | 0.5296   | 0.7095  |\n",
        "| True            | 128          | 128        | 1        | 0.5277   | 0.6932  |\n",
        "| True            | 128          | 128        | 5        | 0.5378   | 0.7127  |\n",
        "| True            | 128          | 128        | 10       | 0.5223   | 0.7010  |\n",
        "| True            | 128          | 256        | 1        | 0.5528   | 0.6747  |\n",
        "| True            | 128          | 256        | 5        | 0.5322   | 0.6990  |\n",
        "| True            | 128          | 256        | 10       | 0.5634   | 0.6985  |\n",
        "| True            | 256          | 64         | 1        | 0.5084   | 0.7058  |\n",
        "| True            | 256          | 64         | 5        | 0.5452   | 0.6924  |\n",
        "| True            | 256          | 64         | 10       | 0.5431   | 0.6989  |\n",
        "| True            | 256          | 128        | 1        | 0.5163   | 0.6990  |\n",
        "| True            | 256          | 128        | 5        | 0.5254   | 0.7063  |\n",
        "| True            | 256          | 128        | 10       | 0.5429   | 0.7020  |\n",
        "| True            | 256          | 256        | 1        | 0.5393   | 0.6861  |\n",
        "| True            | 256          | 256        | 5        | 0.5437   | 0.7048  |\n",
        "| True            | 256          | 256        | 10       | 0.5032   | **0.7293**  |\n",
        "| False           | 64           | 64         | 1        | 0.6235   | 0.6442  |\n",
        "| False           | 64           | 64         | 5        | 0.5612   | 0.6639  |\n",
        "| False           | 64           | 64         | 10       | 0.5926   | 0.6511  |\n",
        "| False           | 64           | 128        | 1        | 0.5550   | 0.6845  |\n",
        "| False           | 64           | 128        | 5        | 0.5499   | 0.6678  |\n",
        "| False           | 64           | 128        | 10       | 0.5295   | 0.6959  |\n",
        "| False           | 64           | 256        | 1        | 0.5287   | 0.6867  |\n",
        "| False           | 64           | 256        | 5        | 0.5322   | 0.6776  |\n",
        "| False           | 64           | 256        | 10       | 0.5384   | 0.6786  |\n",
        "| False           | 128          | 64         | 1        | 0.6041   | 0.6628  |\n",
        "| False           | 128          | 64         | 5        | 0.5792   | 0.6787  |\n",
        "| False           | 128          | 64         | 10       | 0.5609   | 0.7026  |\n",
        "| False           | 128          | 128        | 1        | 0.5598   | 0.7038  |\n",
        "| False           | 128          | 128        | 5        | 0.5423   | 0.7026  |\n",
        "| False           | 128          | 128        | 10       | 0.5559   | 0.6944  |\n",
        "| False           | 128          | 256        | 1        | 0.5477   | 0.6981  |\n",
        "| False           | 128          | 256        | 5        | 0.5279   | 0.7123  |\n",
        "| False           | 128          | 256        | 10       | 0.5273   | 0.7142  |\n",
        "| False           | 256          | 64         | 1        | 0.6279   | 0.6513  |\n",
        "| False           | 256          | 64         | 5        | 0.5825   | 0.6852  |\n",
        "| False           | 256          | 64         | 10       | 0.5629   | 0.6921  |\n",
        "| False           | 256          | 128        | 1        | 0.5699   | 0.7061  |\n",
        "| False           | 256          | 128        | 5        | 0.5605   | 0.6947  |\n",
        "| False           | 256          | 128        | 10       | 0.5418   | 0.7067  |\n",
        "| False           | 256          | 256        | 1        | 0.5635   | 0.6986  |\n",
        "| False           | 256          | 256        | 5        | 0.5326   | 0.7038  |\n",
        "| False           | 256          | 256        | 10       | 0.5425   | 0.7061  |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHY543fxXeQz"
      },
      "source": [
        "Results on the kaggle test dataset:\n",
        "\n",
        "* public score (bidirectional, hidden_dim = 256, embedding_dim = 256, n_layers = 10): 0.76248\n",
        "* public score (one directional, hidden_dim = 128, embedding_dim = 256, n_layers = 10): 0.72295"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTTEGZYgc_Fb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
