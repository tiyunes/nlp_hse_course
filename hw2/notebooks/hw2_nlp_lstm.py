# -*- coding: utf-8 -*-
"""hw2_nlp_lstm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zLZ7_Gu-NuTRWrkS9XplmWPszmLlsWZn
"""

import os
import re

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import torch
import torch.nn as nn
import torch.nn.functional as F

from google.colab import drive

drive.mount('/content/drive/')

"""## Dataset

I've chosen [BBC news dataset](https://www.kaggle.com/datasets/pariza/bbc-news-summary). I selected the sports, tech, and business categories, which together comprise 1422 news articles.

Description of the dataset: This dataset was created using a dataset used for data categorization that onsists of 2225 documents from the BBC news website corresponding to stories in five topical areas from 2004-2005 used in the paper of D. Greene and P. Cunningham. "Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering", Proc. ICML 2006; whose all rights, including copyright, in the content of the original articles are owned by the BBC.

Creating dataframe based on txt files
"""

def create_df_from_folder(folder, suffix):
    records = []

    for fname in os.listdir(folder):
        full_path = os.path.join(folder, fname)

        if not fname.lower().endswith('.txt'):
            continue

        with open(full_path, 'r', errors='ignore') as f:
            content = f.read()

        name, ext = os.path.splitext(fname)
        records.append({'file_name': f"{name}_{suffix}{ext}",
                        'text': content,
                        })

    news_df = pd.DataFrame(records)
    return news_df

# sport category

sport_news_folder = r'/content/drive/MyDrive/documents/вшэ/10сем/nlp/homeworks/hw_2/bbc_news/BBC News Summary/News Articles/sport/'

sports_df = create_df_from_folder(sport_news_folder, 'sport')

# tech category

tech_news_folder = r'/content/drive/MyDrive/documents/вшэ/10сем/nlp/homeworks/hw_2/bbc_news/BBC News Summary/News Articles/tech/'

tech_df = create_df_from_folder(tech_news_folder, 'tech')

# business category

business_path = r'/content/drive/MyDrive/documents/вшэ/10сем/nlp/homeworks/hw_2/bbc_news/BBC News Summary/News Articles/business/'

business_df = create_df_from_folder(business_path, 'business')

# combining three news categories

news_df = pd.concat([sports_df,
                     tech_df,
                     business_df,], axis=0)

news_df.shape

avg_n_words = (news_df['text'].apply(lambda x: len(x.split()))).mean()
print(f"average number of words in news dataset = {avg_n_words}")



"""## Tokenizing"""

from pathlib import Path

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

tokenizer

news_df['text'].iloc[0]

tokenizer.decode(tokenizer.encode(news_df['text'].iloc[0]))

dataset_lens = [len(tokenizer.encode(text)) for text in news_df['text'].values]

plt.figure(figsize=(8, 6))

plt.hist(dataset_lens, bins=40)
plt.title('Distribution of lengths for tokenized documents')
plt.legend()

plt.show();

MAX_LENGTH = 1600

# ! pip install wandb

import wandb

"""## Dataset preparation"""

from torch.utils.data import Dataset, DataLoader

from typing import Sequence

class NewsDataset(Dataset):
    def __init__(self, datadirs: Sequence[Path]) -> None:
        self.records = []
        for datadir in datadirs:
            for txtfile in datadir.glob("*.txt"):
                content = txtfile.read_text(errors='ignore')
                self.records.append(content)

    def __getitem__(self, index: int) -> tuple[list[int], int]:
        return self.records[index]

    def __len__(self) -> int:
        return len(self.records)

data_folders = [
    Path("/content/drive/MyDrive/documents/вшэ/10сем/nlp/homeworks/hw_2/bbc_news/BBC News Summary/News Articles/sport/"),
    Path("/content/drive/MyDrive/documents/вшэ/10сем/nlp/homeworks/hw_2/bbc_news/BBC News Summary/News Articles/tech/"),
    Path("/content/drive/MyDrive/documents/вшэ/10сем/nlp/homeworks/hw_2/bbc_news/BBC News Summary/News Articles/business/"),
]

news_dataset = NewsDataset(data_folders)

len(news_dataset)

from torch.utils.data.dataset import random_split
train_dataset, test_dataset = random_split(news_dataset, lengths=[0.9, 0.1])
print("train size: ", len(train_dataset))
print("test size: ", len(test_dataset))

train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)

"""## LSTM in encoder-decoder style architecture from scratch"""

import torch
import torch.nn as nn

class LSTMGeneratorEncDec(nn.Module):
    def __init__(self, vocab_size: int, hidden_size: int, max_len: int, tokenizer):
        super().__init__()
        self.max_len   = max_len
        self.tokenizer = tokenizer

        self.embedding = nn.Embedding(vocab_size, hidden_size)

        self.encoder = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size,
            bidirectional=True,
            batch_first=True
        )

        self.decoder = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size,
            bidirectional=False,
            batch_first=True
        )

        self.h_proj = nn.Linear(2 * hidden_size, hidden_size)
        self.c_proj = nn.Linear(2 * hidden_size, hidden_size)

        self.out_proj = nn.Linear(hidden_size, vocab_size)

    def forward(self, input_ids: torch.LongTensor) -> torch.FloatTensor:
        B, L = input_ids.size()

        emb = self.embedding(input_ids)
        _, (h_enc, c_enc) = self.encoder(emb)

        h_cat = torch.cat([h_enc[0],  h_enc[1]],  dim=1)
        c_cat = torch.cat([c_enc[0],  c_enc[1]],  dim=1)

        h0 = self.h_proj(h_cat).unsqueeze(0)
        c0 = self.c_proj(c_cat).unsqueeze(0)

        dec_out, _ = self.decoder(emb, (h0, c0))

        logits = self.out_proj(dec_out)
        return logits

    @torch.inference_mode()
    def generate(self, input_ids: torch.LongTensor) -> torch.LongTensor:
        B, L0 = input_ids.size()

        emb0 = self.embedding(input_ids)
        _, (h_enc, c_enc) = self.encoder(emb0)

        h_cat = torch.cat([h_enc[0], h_enc[1]], dim=1)
        c_cat = torch.cat([c_enc[0], c_enc[1]], dim=1)
        h, c = (
            self.h_proj(h_cat).unsqueeze(0),
            self.c_proj(c_cat).unsqueeze(0)
        )

        next_token = input_ids[:, -1]

        generated = [input_ids]
        for _ in range(self.max_len):
            emb_tok = self.embedding(next_token).unsqueeze(1)
            out, (h, c) = self.decoder(emb_tok, (h, c))
            logits    = self.out_proj(out)
            next_token = logits.argmax(-1).squeeze(1)
            generated.append(next_token.unsqueeze(1))

        return torch.cat(generated, dim=1)

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
device

len(tokenizer)

model = LSTMGeneratorEncDec(len(tokenizer), 128, MAX_LENGTH, tokenizer).to(device)

print('all parameters:', sum([torch.numel(p) for p in model.parameters()]))

def tokenize(batch):
    tokenizer_kwargs = dict(
        padding=True,
        max_length=256,
        truncation=True,
        return_tensors='pt'
    )
    input = tokenizer(batch, **tokenizer_kwargs).to(device)

    return input

def train(model, dataloader, optimizer):
    model.train()
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)

    for batch in dataloader:
        input = tokenize(batch)

        logits = model(
            input['input_ids'],
            # input['attention_mask'],
        )

        shifted_logits = logits[:, :-1, :]
        shifted_targets = input['input_ids'][:, 1:]

        loss = criterion(shifted_logits.permute(0, 2, 1), shifted_targets)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        accuracy = (shifted_logits.argmax(-1) == shifted_targets).float()
        accuracy = accuracy[shifted_targets != tokenizer.pad_token_id].mean().item()

        wandb.log({
            "train_loss": loss.item(),
            "train_accuracy": accuracy
        })

@torch.inference_mode()
def evaluate(model, dataloader):
    model.eval()
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)

    losses = []
    accuracies = []
    for batch in dataloader:
        inp = tokenize(batch)
        logits = model(inp['input_ids'])
        sl, st = logits[:, :-1, :], inp['input_ids'][:, 1:]
        loss = criterion(sl.permute(0,2,1), st)
        losses.append(loss.item())

        preds = sl.argmax(-1)
        mask  = st != tokenizer.pad_token_id
        acc   = (preds[mask] == st[mask]).float().mean().item()
        accuracies.append(acc)

    avg_loss = sum(losses) / len(losses)
    avg_acc  = sum(accuracies) / len(accuracies)

    wandb.log({
        "val_loss": avg_loss,
        "val_accuracy": avg_acc,
    })

    return avg_loss, avg_acc

for e in range(80):
    train(model, train_dataloader, optimizer)
    evaluate(model, test_dataloader)

from IPython.display import Image, display
display(Image(url="https://raw.githubusercontent.com/tiyunes/nlp_hse_course/main/hw2/images/lstm_enc_dec_train_plots.png"))
print("Plots of training LSTM with encoder-decoder architecture")

lstm_path = '/content/drive/MyDrive/documents/вшэ/10сем/nlp/homeworks/hw_2/lstm.pt'

model.load_state_dict(torch.load(lstm_path))

def clean_text(texts):
    new_texts = []
    for text in texts:
        cls_pos = text.find('[CLS]')
        text = text[cls_pos + len('[CLS] '):]

        sep_pos = text.find(' [SEP]')
        if sep_pos != -1:
            text = text[:sep_pos]

        new_texts.append(text)

    return new_texts

"""2nd LSTM from scratch (without encoder-decoder architecture)"""

import torch
import torch.nn as nn

class LSTMGeneratorLight(nn.Module):
    def __init__(self, vocab_size: int, hidden_size: int, max_len: int, tokenizer):
        super().__init__()
        self.max_len   = max_len
        self.tokenizer = tokenizer

        self.embedding = nn.Embedding(vocab_size, hidden_size)

        self.lstm = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size,
            batch_first=True
        )

        self.out_proj = nn.Linear(hidden_size, vocab_size)

    def forward(self, input_ids: torch.LongTensor) -> torch.FloatTensor:
        emb = self.embedding(input_ids)

        dec_out, _ = self.lstm(emb)

        logits = self.out_proj(dec_out)
        return logits

    @torch.inference_mode()
    def generate(self, input_ids: torch.LongTensor) -> torch.LongTensor:
        device = input_ids.device
        B, L0 = input_ids.size()

        emb0 = self.embedding(input_ids)
        _, (h, c) = self.lstm(emb0)

        next_token = input_ids[:, -1]

        generated = [input_ids]
        for _ in range(self.max_len):
            emb_tok = self.embedding(next_token).unsqueeze(1)
            out, (h, c) = self.lstm(emb_tok, (h, c))
            logits     = self.out_proj(out)
            next_token = logits.argmax(dim=-1).squeeze(1)
            generated.append(next_token.unsqueeze(1))

        return torch.cat(generated, dim=1)

model_light = LSTMGeneratorLight(len(tokenizer), 1024, MAX_LENGTH, tokenizer).to(device)

print('all parameters:', sum([torch.numel(p) for p in model_light.parameters()]))

print('non-embedding parameters:',
      sum([torch.numel(p) for p in model_light.parameters()]) - \
      torch.numel(model_light.embedding.weight) - \
      torch.numel(model_light.out_proj.weight)
)

from tqdm.auto import trange

NUM_EPOCHS = 80
patience   = 3
best_loss  = float('inf')
wait       = 0

for epoch in trange(1, NUM_EPOCHS+1, desc="Epoch"):
    train(model_light, train_dataloader, optimizer_light)
    val_loss, val_acc = evaluate(model_light, test_dataloader)
    print(f"epoch {epoch}: val_loss={val_loss:.4f}, val_acc={val_acc:.4f}")

    if val_loss < best_loss:
        best_loss = val_loss
        wait = 0
    else:
        wait += 1
        if wait >= patience:
            print(f"early stopping at epoch {epoch}")
            break

lstm_light_path = '/content/drive/MyDrive/documents/вшэ/10сем/nlp/homeworks/hw_2/lstm_light.pt'

from IPython.display import Image, display
display(Image(url="https://raw.githubusercontent.com/tiyunes/nlp_hse_course/main/hw2/images/lstm_light_train_plots.png"))
print("Plots of training LSTM without encoder-decoder architecture")

# torch.save(model_light.state_dict(), lstm_light_path)

model_light.load_state_dict(torch.load(lstm_light_path))



"""Evaluating the model"""

from nltk.tokenize import word_tokenize
import math

correct_phrases = [
    "the central bank raised interest rates",
    "the mayor unveiled the new infrastructure plan",
    "researchers reported a breakthrough in cancer treatment",
    "oil prices surged amid supply concerns",
    "the international summit concluded with a joint statement"
]

incorrect_phrases = [
    "bank central the raised rates interest",
    "mayor the plan unveiled infrastructure new the",
    "researchers a in cancer reported breakthrough treatment",
    "surged oil prices amid concerns supply",
    "joint concluded summit international the with statement a"
]

def perplexity_lstm(model, tokenizer, text, device, max_len=256):
    model.eval()

    enc = tokenizer(
        text,
        truncation=True,
        padding="max_length",
        max_length=max_len,
        return_tensors="pt"
    ).to(device)
    input_ids = enc["input_ids"]
    with torch.no_grad():
        logits = model(input_ids)
        shift_logits  = logits[:, :-1, :]
        shift_targets = input_ids[:, 1:]
        loss = F.cross_entropy(
            shift_logits.permute(0,2,1),
            shift_targets,
            ignore_index=tokenizer.pad_token_id
        )
    return math.exp(loss.item())

ppl_light_correct = []

print("Correct phrases for LSTM without encoder-decoder architecture")
for sent in correct_phrases:
    ppl = perplexity_lstm(model_light, tokenizer, sent, device)
    ppl_light_correct.append(ppl)
    print(f" {sent}\nPPL = {ppl}")

ppl_enc_dec_correct = []

print("Correct phrases for LSTM with encoder-decoder architecture")
for sent in correct_phrases:
    ppl = perplexity_lstm(model, tokenizer, sent, device)
    ppl_enc_dec_correct.append(ppl)
    print(f"{sent}\nPPL = {ppl}")

ppl_light_incorrect = []

print("Incorrect phrases for LSTM without encoder-decoder architecture")
for sent in incorrect_phrases:
    ppl = perplexity_lstm(model_light, tokenizer, sent, device)
    ppl_light_incorrect.append(ppl)
    print(f"{sent}\nPPL = {ppl}")

ppl_enc_dec_incorrect = []

print("Incorrect phrases for LSTM with encoder-decoder architecture")
for sent in incorrect_phrases:
    ppl = perplexity_lstm(model, tokenizer, sent, device)
    ppl_enc_dec_incorrect.append(ppl)
    print(f"{sent}\nPPL = {ppl}")

print(f"average perplexity on correct phrases for LSTM without encoder-decoder architecture: {np.mean(ppl_light_correct)}")
print(f"average perplexity on incorrect phrases for LSTM without encoder-decoder architecture: {np.mean(ppl_light_incorrect)}")

print(f"average perplexity on correct phrases for LSTM with encoder-decoder architecture: {np.mean(ppl_enc_dec_correct)}")
print(f"average perplexity on incorrect phrases for LSTM with encoder-decoder architecture: {np.mean(ppl_enc_dec_incorrect)}")

prompts = [
    "I think",
    "She goes to",
    "In the future",
    "Blue dog",
    "Once upon a time"
]

print("Generating for LSTM without encoder-decoder architecture")

for p in prompts:
    inp = tokenizer(p, return_tensors="pt").to(device)
    gen = model_light.generate(inp["input_ids"])
    out = tokenizer.decode(gen[0], skip_special_tokens=True)
    print(f"Prompt: {p}\n {out}")
    print("\n")

print("Generating for LSTM with encoder-decoder architecture")
for p in prompts:
    inp = tokenizer(p, return_tensors="pt").to(device)
    gen = model.generate(inp["input_ids"])
    out = tokenizer.decode(gen[0], skip_special_tokens=True)
    print(f"Prompt: {p}\n{out}")
    print("\n")

